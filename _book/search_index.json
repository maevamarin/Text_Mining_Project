[["index.html", "Dicours Analysis Chapter 1 Basic introduction", " Dicours Analysis Eug√©nie Mathieu, Maeva Marin, Hadrien Renger, Wajma Nazim 14 December, 2020 Chapter 1 Basic introduction "],["introduction.html", "Chapter 2 Introduction 2.1 Overview and Motivation 2.2 Data loadind", " Chapter 2 Introduction 2.1 Overview and Motivation 2.2 Data loadind # Boris Johnson&#39;s speech of March 16th boris16mars &lt;- read_html(&quot;https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-16-march-2020&quot;)%&gt;% html_nodes(xpath=&quot;//*[@id=&#39;content&#39;]/div[3]/div[1]/div[1]/div[2]/div&quot;) %&gt;% html_text() macron &lt;- read_html(&quot;https://franceintheus.org/spip.php?article9654&quot;) %&gt;% html_nodes(&quot;div.texte&quot;) %&gt;% html_text() "],["eda.html", "Chapter 3 EDA 3.1 Data Acquisition 3.2 Tokenisation, Lemmatization &amp; Cleaning 3.3 Document-Term Matrix DTM 3.4 TF-IDF 3.5 Cloud of Words 3.6 Lexical Divesity Token Type Ratio TTR 3.7 Zipf's Law 3.8 Yule's index 3.9 MATTR", " Chapter 3 EDA 3.1 Data Acquisition To download the different speaches, we scrap the speaches from two different websites. The speaches from Macron come from the website of the Embassy of France in Washington DC (USA) and regarding Boris Johnson's speaches, they come from the official website of the governement of the United-Kingdom. 3.1.1 Emmanuel Macron We choose the 3 first speaches from Macron about the corona virus dating from the: * 12 March (text1) * 16 March (text2) * 13 April (text3) Table 3.1: Macron's speeches Text Types Tokens Sentences text1 1007 3772 162 text2 917 2947 107 text3 1103 3730 107 The first speach of Macron was quite long , 162 sentences and the two next were shorter: 107 sentences. Each speech consists approximatly of an average of 3200 words. 3.1.2 Boris Johnson We choose the 7 first speaches of president Johnson about the corona virus dating from the: * 09 March (text1) * 12 March (text2) * 16 March (text3) * 18 March (text5) * 19 March (text6) * 20 March (text7) * 22 March (text8) Table 3.2: Johnson's speeches Text Types Tokens Sentences text1 266 577 21 text2 409 1170 49 text3 405 1165 42 text4 406 1175 51 text5 321 904 1 text6 357 946 42 text7 300 729 33 Johnson made more speeches but shorter. His first speech was 609 words, then the following ones ranged from 793 to 1222 words. 3.2 Tokenisation, Lemmatization &amp; Cleaning Numbers, punctuation, symbols and separators are removed, as well as unimportant words. Moreover, we cast all letters to their corresponding lower case version. 3.2.1 Emmanuel Macron #&gt; Tokens consisting of 3 documents. #&gt; text1 : #&gt; [1] &quot;man&quot; &quot;woman&quot; &quot;france&quot; &quot;dear&quot; &quot;compatriot&quot; #&gt; [6] &quot;past&quot; &quot;week&quot; &quot;country&quot; &quot;confront&quot; &quot;spread&quot; #&gt; [11] &quot;virus&quot; &quot;covid-19&quot; #&gt; [ ... and 1,718 more ] #&gt; #&gt; text2 : #&gt; [1] &quot;woman&quot; &quot;man&quot; &quot;france&quot; &quot;thursday&quot; &quot;night&quot; #&gt; [6] &quot;speak&quot; &quot;health&quot; &quot;crisis&quot; &quot;country&quot; &quot;confront&quot; #&gt; [11] &quot;point&quot; &quot;epidemic&quot; #&gt; [ ... and 1,314 more ] #&gt; #&gt; text3 : #&gt; [1] &quot;frenchwoman&quot; &quot;frenchman&quot; &quot;dear&quot; &quot;compatriot&quot; #&gt; [5] &quot;live&quot; &quot;difficult&quot; &quot;times.we&quot; &quot;feel&quot; #&gt; [9] &quot;fear&quot; &quot;distress&quot; &quot;right&quot; &quot;now&quot; #&gt; [ ... and 1,690 more ] 3.2.2 Boris Johnson #&gt; Tokens consisting of 7 documents. #&gt; text1 : #&gt; [1] &quot;morning&quot; &quot;chair&quot; &quot;meet&quot; &quot;government&#39;s&quot; #&gt; [5] &quot;cobr&quot; &quot;emergency&quot; &quot;committee&quot; &quot;coronavirus&quot; #&gt; [9] &quot;outbreak&quot; &quot;first&quot; &quot;minister&quot; &quot;scotland&quot; #&gt; [ ... and 246 more ] #&gt; #&gt; text2 : #&gt; [1] &quot;good&quot; &quot;afternoon&quot; &quot;everybody&quot; &quot;thank&quot; #&gt; [5] &quot;much&quot; &quot;come&quot; &quot;just&quot; &quot;chair&quot; #&gt; [9] &quot;meet&quot; &quot;government&#39;s&quot; &quot;emergency&quot; &quot;committee&quot; #&gt; [ ... and 501 more ] #&gt; #&gt; text3 : #&gt; [1] &quot;good&quot; &quot;afternoon&quot; &quot;everybody&quot; &quot;thank&quot; &quot;much&quot; #&gt; [6] &quot;come&quot; &quot;want&quot; &quot;bring&quot; &quot;everyone&quot; &quot;date&quot; #&gt; [11] &quot;national&quot; &quot;fight&quot; #&gt; [ ... and 477 more ] #&gt; #&gt; text4 : #&gt; [1] &quot;good&quot; &quot;afternoon&quot; &quot;thank&quot; &quot;come&quot; &quot;indeed&quot; #&gt; [6] &quot;tune&quot; &quot;daily&quot; &quot;update&quot; &quot;want&quot; &quot;introduce&quot; #&gt; [11] &quot;sure&quot; &quot;know&quot; #&gt; [ ... and 532 more ] #&gt; #&gt; text5 : #&gt; [1] &quot;want&quot; &quot;begin&quot; &quot;thank&quot; &quot;everyone&quot; &quot;thank&quot; #&gt; [6] &quot;medium&quot; &quot;also&quot; &quot;thank&quot; &quot;everyone&quot; &quot;huge&quot; #&gt; [11] &quot;effort&quot; &quot;country&quot; #&gt; [ ... and 356 more ] #&gt; #&gt; text6 : #&gt; [1] &quot;good&quot; &quot;afternoon&quot; &quot;thank&quot; &quot;come&quot; &quot;today&quot; #&gt; [6] &quot;join&quot; &quot;chancellor&quot; &quot;exchequer&quot; &quot;rishi&quot; &quot;sunak&quot; #&gt; [11] &quot;jennie&quot; &quot;harry&quot; #&gt; [ ... and 392 more ] #&gt; #&gt; [ reached max_ndoc ... 1 more document ] 3.3 Document-Term Matrix DTM Now let's compute the word frequencies (TF) by documents. First, the tokens are grouped by the indicator Document, which allows to count the words by documents. 3.3.1 Table Emmanuel Macron Boris Johnson 3.3.2 Most frequent words We only keep the 15 most frequent words for redability purpose and create barplots using ggplot and geom_col. The facetwrap function split the barplots per Document. 3.3.2.1 All text confused Emmanuel Macron Boris Johnson We see that the list of the 15 most frequent terms is due to doc 2 ,4 and 3. 3.3.2.2 Per text Now we want to know which are the most frequent terms for each speach. Emmanuel Macron Boris Johnson 3.4 TF-IDF Now we repeat the same analysis using the TF-IDF. 3.4.1 Emmanuel Macron 3.4.2 Boris Johnson 3.5 Cloud of Words 3.5.1 Usind DFM Emmanuel macron Boris Johnson 3.5.2 Using TF-IDF Emmanuel macron Boris Johnson 3.6 Lexical Divesity Token Type Ratio TTR A TTR is comprised beetween o and 1.When equal to 1 it correspond to a rich diversity, every token is from a different type. In opposit if equal to 0, it mean a poor diversity ( he use only one word). A TTR is comprised beetween 0 and 1. When equal to 1, it corresponds to a rich lexical diversity, this is to say that each token is from a different type. In opposite, if equal to 0, it mean that the corpus presents a poor lexical diversity (if he would use one word only). 3.6.1 Emmanuel Macron Table 3.3: Lexical diversity of Macron. x text1 0.413 text2 0.493 text3 0.489 Macron has a mean TTR of 0,45, which is quite poor. 3.6.2 Boris Johnson Table 3.4: Lexical diversity of Johnson. x text1 0.674 text2 0.548 text3 0.558 text4 0.509 text5 0.576 text6 0.611 text7 0.611 Johnson has quite a richer vocabulary, an average of 0,6 over the different corpuses. 3.7 Zipf's Law Now, we illustrate the Zipf's law on the discourses. The terms are ranked by their corresponding frequency (rank=1 for the most frequent), then plotted versus tehir rank. This is easily obtained using quanteda. Using a log-log relation, this gives us a linear regression. 3.7.1 Emmanuel Macron 3.7.2 Boris Johnson 3.8 Yule's index A larger index means more diversity. 3.8.1 Emmanuel Macron 3.8.2 Boris Johnson 3.9 MATTR It is the Moving Average Type-Token Ratio. 3.9.1 Emmanuel Macron 3.9.2 Boris Johnson "],["sentiment-analysis.html", "Chapter 4 Sentiment Analysis 4.1 Analysis with the &quot;nrc&quot; library 4.2 Analysis with the LSD2015 dictionnary 4.3 Analysis with the &quot;afinn&quot; dictionnary", " Chapter 4 Sentiment Analysis 4.1 Analysis with the &quot;nrc&quot; library We use the &quot;nrc&quot; dictionary to start our sentiment analysis on the discourses of our two politicians. It is a dictionnary qualifying tokens by specific sentiments and by labelling them &quot;negative&quot; or &quot;positive&quot;. To do so, we will match the tokens of both corpuses with the dictionnary by applying an inner join. However, to use the inner_join function, we need a table object, what our objects are not primarily. We reload the data to create objects specific to this stage, boris_2 and macron_2, which are registered as tibble and allowing the use of the inner_join function. From the token list per document boris.tok, we join the corresponding qualifier in nrc using an inner_joint: ##################################################################################################################################### ########################################## Let&#39;s start with the Boris Johnson&#39;s discourses ###################################### ##################################################################################################################################### boris_2&lt;-as.tibble( c(boris9mars, boris12mars, boris16mars, boris18mars, boris19mars, boris20mars, boris22mars)) # trick to get a &quot;tbl_df&quot;,&quot;tbl&quot;,&quot;data.frame&quot; compatible with the inner_join function DocumentB &lt;- c(&quot;Text1&quot;,&quot;Text2&quot;,&quot;Text3&quot;,&quot;Text4&quot;,&quot;Text5&quot;,&quot;Text6&quot;,&quot;Text7&quot;) # adding a column &quot;Document&quot; to have a landmark for the tokens boris_2$Document &lt;- DocumentB boris_2 &lt;- boris_2[,c(2,1)] boris_2.tok &lt;- unnest_tokens(boris_2, output=&quot;word&quot;, input=&quot;value&quot;, to_lower=TRUE, strip_punct=TRUE, strip_numeric=TRUE) # unnest tokens of the table boris_2.sent&lt;- boris_2.tok %&gt;% inner_join(get_sentiments(&quot;nrc&quot;)) # do the inner join to merge the two tables ###################################################################################################################################### ########################################## Let&#39;s continue with the Macron&#39;s discourses ########################################### ###################################################################################################################################### macron_2&lt;-as.tibble( c(macron12march, macron13april, macron16march)) # trick to get a &quot;tbl_df&quot;,&quot;tbl&quot;,&quot;data.frame&quot; compatible with the inner_join function DocumentM &lt;- c(&quot;Text1&quot;,&quot;Text2&quot;,&quot;Text3&quot;) # adding a column &quot;Document&quot; to have a landmark for the tokens macron_2$Document &lt;- DocumentM macron_2 &lt;- macron_2[,c(2,1)] macron_2.tok &lt;- unnest_tokens(macron_2, output=&quot;word&quot;, input=&quot;value&quot;, to_lower=TRUE, strip_punct=TRUE, strip_numeric=TRUE) # unnest tokens of the table macron_2.sent&lt;- macron_2.tok %&gt;% inner_join(get_sentiments(&quot;nrc&quot;)) # do the inner join to merge the two tables After creating our objects, we investigate which sentiment are present in Boris Johnson's discourses. To do so, we use a numerical and a graphical method. The numerical method is simply a matrix of the frequency of tokens identified to a certain sentiment. Remind that a word can have more than 1 sentiment, which can lead to slight an overestimation of the sentiment. Below, the table ?? presents positive discourses from the UK's First Minister which are mainly weighted by the sentiment &quot;trust&quot;. We could have assumed that the discourse would be reassuring in order to avoid any panic due to the inedite circumstances of the covid. &quot;Anticipation&quot; is as well high, for the same reason (annoucement of the futures measures ans aaniticpations of the consequences, e.g.) However, postive sentiments are balanced by the relative high score of the fear, followed by disgust and anger. In absolute values, the most sentimental discourse was the second public word, on March 12th 2020. The graphical representations of the sentiments among discourses enables us a quick glimpse on these results. table(boris_2.sent$Document,boris_2.sent$sentiment) %&gt;% kable() %&gt;% kable_styling() # sentiment terms per document anger anticipation disgust fear joy negative positive sadness surprise trust Text1 7 14 5 15 4 14 33 7 4 24 Text2 16 33 17 33 8 35 63 22 7 45 Text3 12 16 14 25 10 23 46 15 8 31 Text4 11 24 8 14 9 20 50 11 8 35 Text5 11 18 10 19 8 24 30 12 5 19 Text6 8 17 6 14 9 22 45 8 2 35 Text7 3 13 4 12 8 13 26 4 3 23 The graphical representation of the sentiments among discourses enables us a quick glimpse on these results. boris_2.sent %&gt;% group_by(Document,sentiment) %&gt;% summarize(n=n())%&gt;% mutate(freq=n/sum(n)) %&gt;% ggplot(aes(x=sentiment,y=freq,fill=sentiment)) + geom_bar(stat=&quot;identity&quot;,alpha=0.8) + facet_wrap(~ Document) + coord_flip() + ggtitle(&quot;Boris Johnson: Graphical representation of the sentiment per text&quot;) + xlab(&quot;Frequencies of the sentiments&quot;) + ylab(&quot;Sentiment&quot;) + geom_text(aes(label = n), size = 3, hjust = 1, vjust = 0, position = &quot;stack&quot;) Figure 4.1: Boris Johnson: Graphical representation of the sentiment per text By looking now at the discourses of Macron using the same method, we note the seemingly same frequencies of sentiments in the table ??. His words appear to be carefully built and the variance is very low, perhaps to give the impression to have a stable and coherent speeches overtime. The categories of sentiments are quite similarly distributed: the speeches are positive marked by trust and anticipation. table(macron_2.sent$Document,macron_2.sent$sentiment) %&gt;% kable() %&gt;% kable_styling() # sentiment terms per document anger anticipation disgust fear joy negative positive sadness surprise trust Text1 25 88 15 72 39 109 193 42 33 121 Text2 27 89 19 60 39 100 174 43 28 100 Text3 25 62 10 52 24 91 147 30 22 100 macron_2.sent %&gt;% group_by(Document,sentiment) %&gt;% summarize(n=n())%&gt;% mutate(freq=n/sum(n)) %&gt;% ggplot(aes(x=sentiment,y=freq,fill=sentiment)) + geom_bar(stat=&quot;identity&quot;,alpha=0.8) + facet_wrap(~ Document) + coord_flip() + ggtitle(&quot;Graphical representation of the sentiment per text&quot;) + xlab(&quot;Frequencies of the sentiments&quot;) + ylab(&quot;Sentiment&quot;) + geom_text(aes(label = n), size =3, hjust = 1, vjust = 0, position = &quot;stack&quot;) 4.2 Analysis with the LSD2015 dictionnary In order to better analyse those results and fortify those insights, we double check with the dictionnary LSD2015. It is another dictionnary assigning a qualifier to terms. The difference with tidytext is essentially in the manipulation of the objects: it handles the &quot;tokens&quot; class and for this reason we have to recode our objects. The results are impacted by this different treatment and we do expect slight changes. As the figure 4.2 shows, the results of Text 2 as the most positive and all the discourses as majoritarily positive are confirmed. For Macron's speeches, the proportions of sentiments differs: the first discourse is associated with more postive sentiment (224) relatively to proportion computed with the &quot;nrc&quot; dictionnary (193). However, the trend remains the same. We keep in mind that difference in length of texts explains the varying size of the bar, as previously. Anyway both findings converge to the same points. ###################################################################################################################################### ########################################## Let&#39;s start with the Johnson&#39;s discourses ########################################### ###################################################################################################################################### boris.cp&lt;-corpus(c(boris9mars,boris12mars,boris16mars,boris18mars,boris19mars,boris20mars,boris22mars)) summary(boris.cp) Text Types Tokens Sentences text1 266 577 21 text2 409 1170 49 text3 405 1165 42 text4 406 1175 51 text5 321 904 1 text6 357 946 42 text7 300 729 33 boris.tk&lt;-tokens(boris.cp, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE) boris.tk&lt;-tokens_tolower(boris.tk) boris.tk&lt;- tokens_replace(boris.tk,pattern = hash_lemmas$token, replacement = hash_lemmas$lemma) boris.tk&lt;-boris.tk %&gt;% tokens_remove(stopwords(&quot;english&quot;)) boris.sent&lt;- tokens_lookup(boris.tk,dictionary = data_dictionary_LSD2015) %&gt;% dfm() %&gt;% tidy boris.plot.quanteda &lt;- ggplot(boris.sent, aes( x = document, y = count, fill = term)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + ggtitle(&quot;Johnson: Proportion of sentiment using the dictionnary LSD2015&quot;) + xlab(&quot;Document&quot;) + ylab(&quot;Number of terms attributed to negative and positive sentiments&quot;) + geom_text(aes(label = count), size =3, hjust = 5, vjust = 0, position = &quot;stack&quot;) ###################################################################################################################################### ########################################## Let&#39;s continue with the Macron&#39;s discourses ########################################### ###################################################################################################################################### macron.cp&lt;-corpus(c(macron12march,macron13april,macron16march)) macron.tk&lt;-tokens(macron.cp, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE) macron.tk&lt;-tokens_tolower(macron.tk) macron.tk&lt;- tokens_replace(macron.tk,pattern = hash_lemmas$token, replacement = hash_lemmas$lemma) macron.tk&lt;-macron.tk %&gt;% tokens_remove(stopwords(&quot;english&quot;)) macron.sent&lt;- tokens_lookup(macron.tk,dictionary = data_dictionary_LSD2015) %&gt;% dfm() %&gt;% tidy macron.plot.quanteda&lt;- ggplot(macron.sent, aes( x = document, y = count, fill = term)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + ggtitle(&quot;Macron: Proportion of sentiment using the dictionnary LSD2015&quot;) + xlab(&quot;Document&quot;) + ylab(&quot;Number of terms attributed to negative and positive sentiments&quot;) + geom_text(aes(label = count), size =3, hjust = 5, vjust = 0, position = &quot;stack&quot;) grid.arrange(boris.plot.quanteda,macron.plot.quanteda) Figure 4.2: Proportion of sentiment using the dictionnary LSD2015 4.3 Analysis with the &quot;afinn&quot; dictionnary We now use a different approach: a quantitative way to assess the sentiment analysis. To do so, we use the &quot;afinn&quot; dictionnary which attributes a value to the word, taking into account the power conveyed by the term (value between 0 and 1) and its qualitative classification (positive or neagtive sign). The classification of the words was encoded differently than for the &quot;nrc&quot; dictionnary. Again, we do expect different scores, but hopefully in the same direction. The results displayed by the figure 4.3 are derived from an average score of sentiment per document. For the Boris Johnson's speeches, the second one has a contradictory score with the previous results: it belongs to the lowest scored text in positive sentiments. We can thus observe the difference in encodage among dictionnaries used in Text Mining. The &quot;afinn&quot; dictionnarywas encoded by a Danish Professor, LSD2015 by two American professors, and &quot;nrc&quot; by a Canadian professors working in an Commision on Ethics. The categorization is subjective and lead to substantial differences. Those results need to be mitigate by a last approach: the use of valence shifters. ###################################################################################################################################### ########################################## Let&#39;s continue with the Johnson&#39;s discourses ########################################### ###################################################################################################################################### boris_2.sent &lt;- boris_2.tok %&gt;% inner_join(get_sentiments(&quot;afinn&quot;)) boris.plot.afinn&lt;-aggregate(value~Document, data =boris_2.sent,FUN=mean) %&gt;% ggplot(aes(x=Document,y=value, fill = Document)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() + ggtitle(&quot;Johnson: Sentiment score per text by using afinn dictionnary&quot;) + ylab(&quot;Document&quot;) + xlab(&quot;Score value&quot;) ###################################################################################################################################### ########################################## Let&#39;s continue with the Macron&#39;s discourses ########################################### ###################################################################################################################################### macron_2.sent &lt;- macron_2.tok %&gt;% inner_join(get_sentiments(&quot;afinn&quot;)) macron.plot.afinn &lt;- aggregate(value~Document, data =macron_2.sent,FUN=mean) %&gt;% ggplot(aes(x=Document,y=value, fill = Document)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() + ggtitle(&quot;Macron: Sentiment score per text by using afinn dictionnary&quot;) + ylab(&quot;Document&quot;) + xlab(&quot;Score value&quot;) grid.arrange(boris.plot.afinn,macron.plot.afinn) Figure 4.3: Graphical representation: Sentiment score per text by using afinn dictionnary 4.3.1 Analysis using &quot;nrc&quot;&quot; dictionnary and valence shifters The sentimentr library offers some function to compute sentiments integrating valence shiters. There are specific words which amplify or reduce the power of a word, even turn it into the reverse sentiment. We evaluate sentences here, and one important aspect is that it cannot be applied to a Bag Of Word model, since the word order is necessary. The valence shifters extract more acurately the sentiment ouf of the text, since it consider the sentiment conveyed by a sentence and not only words without context. We should take the insights given by valence shifters as the ultimate confirmation test for the previous results. The results are displayed by the plot 4.4. The second discourse of the Prime Minister here is one of the longest: it contains about 50 sentences and is only preceded by the fourth public word, on March 18th. It is the one varying the most in sentiment, what might explains the dffference in values observed between LSD2015&amp;nrc with afinn dictionnary. The sixth one shows one positive peak, what correspond to the moment whn Boris Johnson strengthened the measures and made a speech to reassure UK citizen. He brought information with positive sentiments to unify the population under the new circumstances and to push the to obey by showing them the positive impact of respecting those stonger measures. Macron's speeches are well longer, three times more than Boris Johnson's ones, as we observe in the second plot. His words are generally more neutral but convey sometimes strong negative sentiments, like the 4th sentence. Actually, this sentence is not strongly negative but uses terms which are: &quot;In the vast majority of cases, COVID-19 does not pose a threat, but the virus can have very serious consequences, especially for those of our fellow citizens who are elderly or suffer from chronic illnesses such as diabetes, obesity or cancer&quot;. The variation of the sentiments is greater in the last allocution, on April 13th 2020. sentiment(boris.text, question.weight = 0) boris.text&lt;-get_sentences(boris) boris.senti&lt;-sentiment(boris.text) sentiment(boris.text, question.weight = 0) element_id sentence_id word_count sentiment 1 1 17 -0.497 1 2 34 0.334 1 3 15 0.258 1 4 12 0.217 1 5 22 0.000 1 6 23 -0.209 1 7 45 -0.224 1 8 23 -0.386 1 9 22 -0.032 1 10 32 0.101 1 11 27 -0.048 1 12 22 0.000 1 13 30 -0.243 1 14 27 0.953 1 15 20 0.089 1 16 27 0.125 1 17 35 0.122 1 18 22 0.213 1 19 25 -0.198 1 20 32 0.424 1 21 20 0.000 2 1 10 0.522 2 2 20 -0.145 2 3 24 0.184 2 4 18 -0.118 2 5 7 -0.038 2 6 32 0.044 2 7 24 0.031 2 8 7 -0.132 2 9 5 -0.537 2 10 11 -1.116 2 11 30 0.228 2 12 18 0.448 2 13 21 0.164 2 14 12 0.000 2 15 26 -0.968 2 16 44 0.232 2 17 24 -0.041 2 18 11 0.000 2 19 13 0.222 2 20 39 -0.304 2 21 18 -0.109 2 22 9 0.200 2 23 39 -0.244 2 24 22 0.341 2 25 35 0.135 2 26 25 0.050 2 27 14 0.107 2 28 25 0.080 2 29 17 -0.728 2 30 31 0.045 2 31 21 0.349 2 32 8 0.000 2 33 15 0.103 2 34 18 -0.236 2 35 12 0.000 2 36 5 0.000 2 37 19 -0.252 2 38 16 0.038 2 39 13 0.333 2 40 55 -0.302 2 41 12 0.000 2 42 33 -0.736 2 43 34 -0.026 2 44 16 0.125 2 45 25 -0.100 2 46 35 0.617 2 47 31 0.636 2 48 35 0.536 2 49 51 0.070 3 1 9 0.550 3 2 33 0.052 3 3 35 0.101 3 4 12 0.260 3 5 26 -0.078 3 6 32 0.442 3 7 10 -0.047 3 8 32 0.177 3 9 30 0.210 3 10 13 0.000 3 11 26 -0.020 3 12 27 -0.115 3 13 21 -0.087 3 14 12 0.072 3 15 12 -0.144 3 16 16 0.000 3 17 8 0.354 3 18 25 0.230 3 19 17 0.000 3 20 7 0.000 3 21 19 -0.229 3 22 44 -0.077 3 23 14 -0.241 3 24 38 0.143 3 25 43 -0.195 3 26 22 0.128 3 27 23 0.073 3 28 12 0.046 3 29 56 0.414 3 30 30 -0.228 3 31 27 0.520 3 32 29 -0.056 3 33 21 0.000 3 34 10 -0.079 3 35 31 0.081 3 36 38 -0.162 3 37 58 0.223 3 38 11 -0.377 3 39 48 0.180 3 40 11 0.000 3 41 17 0.303 3 42 46 0.258 4 1 2 0.530 4 2 13 0.166 4 3 26 0.127 4 4 16 0.000 4 5 33 -0.061 4 6 12 0.000 4 7 26 0.446 4 8 14 -0.107 4 9 12 -0.208 4 10 20 0.179 4 11 19 -0.195 4 12 3 0.000 4 13 28 0.104 4 14 22 0.399 4 15 22 -0.053 4 16 26 0.569 4 17 23 0.115 4 18 25 -0.300 4 19 12 0.390 4 20 14 -0.601 4 21 29 -0.093 4 22 27 0.486 4 23 35 -0.338 4 24 40 0.103 4 25 12 0.130 4 26 21 0.098 4 27 24 0.000 4 28 15 0.168 4 29 40 0.300 4 30 15 0.000 4 31 23 -0.104 4 32 12 -0.260 4 33 26 -0.245 4 34 21 -0.257 4 35 23 0.271 4 36 16 0.325 4 37 19 0.298 4 38 18 0.000 4 39 18 0.153 4 40 14 -0.214 4 41 21 -0.055 4 42 53 0.240 4 43 44 -0.475 4 44 13 0.000 4 45 17 0.121 4 46 31 0.521 4 47 19 -0.138 4 48 18 0.377 4 49 23 -0.209 4 50 16 0.631 4 51 14 0.000 5 1 866 0.394 6 1 27 0.356 6 2 17 0.024 6 3 8 0.283 6 4 8 0.000 6 5 35 0.118 6 6 25 -0.522 6 7 47 0.117 6 8 7 -0.189 6 9 8 0.088 6 10 4 0.000 6 11 6 0.102 6 12 6 -0.204 6 13 19 -0.023 6 14 49 0.129 6 15 9 0.267 6 16 22 -0.085 6 17 11 -0.024 6 18 19 -0.057 6 19 14 -0.134 6 20 14 -0.027 6 21 10 0.000 6 22 50 -0.141 6 23 27 0.106 6 24 4 0.000 6 25 30 0.571 6 26 29 0.167 6 27 20 -0.045 6 28 15 0.000 6 29 39 0.296 6 30 12 0.491 6 31 17 0.194 6 32 21 0.000 6 33 18 -0.265 6 34 23 -0.156 6 35 19 0.000 6 36 4 1.125 6 37 58 -0.038 6 38 22 0.213 6 39 18 0.424 6 40 6 0.000 6 41 12 -0.144 6 42 32 0.221 7 1 25 0.350 7 2 19 0.156 7 3 34 0.454 7 4 15 0.000 7 5 19 0.000 7 6 10 0.032 7 7 30 0.037 7 8 12 0.217 7 9 26 -0.245 7 10 40 0.134 7 11 27 -0.096 7 12 12 0.361 7 13 15 0.000 7 14 7 0.000 7 15 13 0.000 7 16 19 0.115 7 17 7 0.000 7 18 12 -0.577 7 19 23 0.000 7 20 13 0.250 7 21 19 -0.138 7 22 36 0.000 7 23 30 0.301 7 24 18 0.648 7 25 15 0.000 7 26 26 -0.206 7 27 19 -0.041 7 28 35 0.237 7 29 21 -0.109 7 30 16 -0.312 7 31 29 0.262 7 32 14 0.000 7 33 8 0.000 boris.senti&lt;-as_tibble(boris.senti) boris.senti element_id sentence_id word_count sentiment 1 1 17 -0.497 1 2 34 0.334 1 3 15 0.258 1 4 12 0.217 1 5 22 0.000 1 6 23 -0.209 1 7 45 -0.224 1 8 23 -0.386 1 9 22 -0.032 1 10 32 0.101 1 11 27 -0.048 1 12 22 0.000 1 13 30 -0.243 1 14 27 0.953 1 15 20 0.089 1 16 27 0.125 1 17 35 0.122 1 18 22 0.213 1 19 25 -0.198 1 20 32 0.424 1 21 20 0.000 2 1 10 0.522 2 2 20 -0.145 2 3 24 0.184 2 4 18 -0.118 2 5 7 -0.038 2 6 32 0.044 2 7 24 0.031 2 8 7 -0.132 2 9 5 -0.537 2 10 11 -1.116 2 11 30 0.228 2 12 18 0.448 2 13 21 0.164 2 14 12 0.000 2 15 26 -0.968 2 16 44 0.232 2 17 24 -0.041 2 18 11 0.000 2 19 13 0.222 2 20 39 -0.304 2 21 18 -0.109 2 22 9 0.200 2 23 39 -0.244 2 24 22 0.341 2 25 35 0.135 2 26 25 0.050 2 27 14 0.107 2 28 25 0.080 2 29 17 -0.728 2 30 31 0.045 2 31 21 0.349 2 32 8 0.000 2 33 15 0.103 2 34 18 -0.236 2 35 12 0.000 2 36 5 0.000 2 37 19 -0.252 2 38 16 0.038 2 39 13 0.333 2 40 55 -0.302 2 41 12 0.000 2 42 33 -0.736 2 43 34 -0.026 2 44 16 0.125 2 45 25 -0.100 2 46 35 0.617 2 47 31 0.636 2 48 35 0.536 2 49 51 0.070 3 1 9 0.550 3 2 33 0.052 3 3 35 0.101 3 4 12 0.260 3 5 26 -0.078 3 6 32 0.442 3 7 10 -0.047 3 8 32 0.177 3 9 30 0.210 3 10 13 0.000 3 11 26 -0.020 3 12 27 -0.115 3 13 21 -0.087 3 14 12 0.072 3 15 12 -0.144 3 16 16 0.000 3 17 8 0.354 3 18 25 0.230 3 19 17 0.000 3 20 7 -0.340 3 21 19 -0.229 3 22 44 -0.077 3 23 14 -0.241 3 24 38 0.143 3 25 43 -0.195 3 26 22 0.128 3 27 23 0.073 3 28 12 0.046 3 29 56 0.414 3 30 30 -0.228 3 31 27 0.520 3 32 29 -0.056 3 33 21 0.000 3 34 10 -0.079 3 35 31 0.081 3 36 38 -0.162 3 37 58 0.223 3 38 11 -0.377 3 39 48 0.180 3 40 11 0.000 3 41 17 0.303 3 42 46 0.258 4 1 2 0.530 4 2 13 0.166 4 3 26 0.127 4 4 16 0.000 4 5 33 -0.061 4 6 12 0.000 4 7 26 0.446 4 8 14 -0.107 4 9 12 -0.208 4 10 20 0.179 4 11 19 -0.195 4 12 3 0.000 4 13 28 0.104 4 14 22 0.399 4 15 22 -0.053 4 16 26 0.569 4 17 23 0.115 4 18 25 -0.300 4 19 12 0.390 4 20 14 -0.601 4 21 29 -0.093 4 22 27 0.486 4 23 35 -0.338 4 24 40 0.103 4 25 12 0.130 4 26 21 0.098 4 27 24 0.000 4 28 15 0.168 4 29 40 0.300 4 30 15 0.000 4 31 23 -0.104 4 32 12 -0.260 4 33 26 -0.245 4 34 21 -0.257 4 35 23 0.271 4 36 16 0.325 4 37 19 0.298 4 38 18 0.000 4 39 18 0.153 4 40 14 -0.214 4 41 21 -0.055 4 42 53 0.240 4 43 44 -0.475 4 44 13 0.000 4 45 17 0.121 4 46 31 0.521 4 47 19 -0.138 4 48 18 0.377 4 49 23 -0.209 4 50 16 0.631 4 51 14 0.000 5 1 866 0.394 6 1 27 0.356 6 2 17 0.024 6 3 8 0.283 6 4 8 0.000 6 5 35 0.118 6 6 25 -0.522 6 7 47 0.117 6 8 7 -0.189 6 9 8 0.088 6 10 4 0.000 6 11 6 0.102 6 12 6 -0.204 6 13 19 -0.023 6 14 49 0.129 6 15 9 0.267 6 16 22 -0.085 6 17 11 -0.024 6 18 19 -0.057 6 19 14 -0.134 6 20 14 -0.027 6 21 10 0.000 6 22 50 -0.141 6 23 27 0.106 6 24 4 0.000 6 25 30 0.571 6 26 29 0.167 6 27 20 -0.045 6 28 15 0.000 6 29 39 0.296 6 30 12 0.491 6 31 17 0.194 6 32 21 0.000 6 33 18 -0.265 6 34 23 -0.156 6 35 19 0.000 6 36 4 1.125 6 37 58 -0.038 6 38 22 0.213 6 39 18 0.424 6 40 6 0.000 6 41 12 -0.144 6 42 32 0.221 7 1 25 0.350 7 2 19 0.156 7 3 34 0.454 7 4 15 0.000 7 5 19 0.000 7 6 10 0.032 7 7 30 0.037 7 8 12 0.217 7 9 26 -0.245 7 10 40 0.134 7 11 27 -0.096 7 12 12 0.361 7 13 15 0.000 7 14 7 0.000 7 15 13 0.000 7 16 19 0.115 7 17 7 0.000 7 18 12 -0.577 7 19 23 0.000 7 20 13 0.250 7 21 19 -0.138 7 22 36 0.000 7 23 30 0.301 7 24 18 0.648 7 25 15 0.000 7 26 26 -0.206 7 27 19 -0.041 7 28 35 0.237 7 29 21 -0.109 7 30 16 -0.312 7 31 29 0.262 7 32 14 0.000 7 33 8 0.000 boris.senti%&gt;% group_by(element_id) %&gt;% ggplot(aes(x=sentence_id,y=sentiment, col = element_id )) + geom_line() + facet_wrap(~element_id) + ggtitle(&quot;Johnson: Evolution of the sentiments \\nwithin speeches using valence shifters&quot;) + ylab(&quot;Document&quot;) + xlab(&quot;Sentences in speeches&quot;) ###################################################################################################################################### ########################################## Let&#39;s continue with the Macron&#39;s discourses ########################################### ###################################################################################################################################### macron.text&lt;-get_sentences(macron) macron.senti&lt;-sentiment(macron.text) sentiment(macron.text, question.weight = 0) element_id sentence_id word_count sentiment 1 1 34 -0.103 1 2 20 0.034 1 3 24 -0.131 1 4 41 -1.325 1 5 12 0.072 1 6 27 0.000 1 7 29 -0.371 1 8 19 0.115 1 9 52 0.653 1 10 10 0.395 1 11 24 -0.020 1 12 12 -0.447 1 13 24 0.388 1 14 7 0.189 1 15 60 0.400 1 16 7 0.000 1 17 10 0.111 1 18 8 -0.354 1 19 42 0.108 1 20 5 0.000 1 21 12 0.361 1 22 15 0.129 1 23 37 0.329 1 24 26 -0.098 1 25 8 -0.265 1 26 15 -0.426 1 27 11 0.754 1 28 37 0.214 1 29 34 -0.054 1 30 19 -0.133 1 31 37 -0.378 1 32 4 0.000 1 33 16 -0.125 1 34 8 0.230 1 35 31 -0.404 1 36 29 0.183 1 37 21 0.000 1 38 17 -0.424 1 39 26 0.255 1 40 17 0.094 1 41 16 0.250 1 42 17 0.061 1 43 34 0.120 1 44 28 0.546 1 45 18 0.377 1 46 8 -0.141 1 47 1 0.000 1 48 32 -0.194 1 49 36 0.108 1 50 61 0.045 1 51 17 0.061 1 52 49 0.507 1 53 21 -0.055 1 54 13 0.139 1 55 15 0.026 1 56 9 0.000 1 57 18 0.012 1 58 22 -0.128 1 59 13 -0.208 1 60 26 0.475 1 61 8 0.194 1 62 18 0.236 1 63 8 0.141 1 64 8 0.354 1 65 6 0.000 1 66 6 0.408 1 67 64 0.325 1 68 27 0.327 1 69 6 0.000 1 70 21 0.404 1 71 64 0.388 1 72 9 0.000 1 73 22 0.927 1 74 20 0.637 1 75 21 0.109 1 76 4 0.300 1 77 26 0.284 1 78 16 0.462 1 79 9 0.000 1 80 13 0.413 1 81 15 0.103 1 82 15 -0.697 1 83 33 -0.087 1 84 14 -0.267 1 85 18 -0.094 1 86 19 -0.344 1 87 7 0.076 1 88 16 -0.088 1 89 30 -0.037 1 90 19 0.344 1 91 15 0.207 1 92 8 0.000 1 93 7 0.000 1 94 10 -0.126 1 95 23 0.849 1 96 36 0.133 1 97 12 0.217 1 98 14 0.347 1 99 28 0.283 1 100 25 -0.550 1 101 11 0.075 1 102 20 0.000 1 103 10 0.237 1 104 7 0.283 1 105 29 0.121 1 106 7 0.302 1 107 10 0.253 1 108 4 0.000 1 109 5 0.000 1 110 11 0.000 1 111 23 0.344 1 112 6 -0.163 1 113 16 0.125 1 114 17 0.158 1 115 9 0.400 1 116 24 0.122 1 117 18 0.460 1 118 24 0.000 1 119 23 0.083 1 120 32 0.256 1 121 20 0.291 1 122 16 0.162 1 123 20 -0.078 1 124 12 0.000 1 125 30 0.228 1 126 18 0.059 1 127 15 0.129 1 128 7 -0.189 1 129 10 0.411 1 130 7 0.397 1 131 10 0.569 1 132 20 0.615 1 133 8 0.000 1 134 12 0.115 1 135 45 0.861 1 136 14 -0.748 1 137 9 0.133 1 138 24 -0.214 1 139 23 0.000 1 140 19 -0.252 1 141 6 0.000 1 142 25 -0.180 1 143 6 -0.408 1 144 13 -0.347 1 145 17 -0.121 1 146 9 0.000 1 147 7 -0.283 1 148 3 0.289 1 149 13 0.277 1 150 14 0.347 1 151 17 0.024 1 152 5 0.179 1 153 11 0.166 1 154 5 0.000 1 155 12 -0.014 1 156 39 0.328 1 157 17 0.061 1 158 15 0.491 1 159 18 0.059 1 160 14 0.388 1 161 7 0.189 1 162 8 0.000 1 163 19 0.344 1 164 16 0.000 1 165 15 -0.129 1 166 24 0.000 1 167 34 0.214 1 168 7 0.000 1 169 27 0.241 1 170 16 0.125 1 171 29 0.093 1 172 22 0.426 1 173 44 0.030 1 174 36 -0.067 1 175 22 -0.085 1 176 19 0.619 1 177 33 0.104 1 178 12 0.000 1 179 6 0.000 1 180 16 0.300 1 181 40 0.032 1 182 15 0.194 1 183 5 0.000 1 184 4 0.000 1 185 3 0.000 2 1 21 -0.164 2 2 23 -0.156 2 3 16 -0.188 2 4 12 0.173 2 5 22 -0.128 2 6 7 -0.302 2 7 17 0.121 2 8 26 0.559 2 9 35 0.355 2 10 7 0.302 2 11 32 0.318 2 12 34 0.086 2 13 21 0.218 2 14 13 0.111 2 15 42 -0.085 2 16 7 0.000 2 17 50 0.404 2 18 11 -0.030 2 19 26 0.029 2 20 24 0.245 2 21 40 0.245 2 22 18 0.000 2 23 41 0.472 2 24 9 0.027 2 25 43 -0.380 2 26 9 0.167 2 27 23 0.031 2 28 10 0.111 2 29 38 0.008 2 30 14 0.000 2 31 18 0.000 2 32 17 0.000 2 33 12 0.043 2 34 75 0.237 2 35 43 0.244 2 36 17 0.255 2 37 10 -0.079 2 38 38 0.105 2 39 6 0.551 2 40 18 0.071 2 41 14 0.167 2 42 12 0.289 2 43 34 -0.026 2 44 15 0.090 2 45 6 0.367 2 46 14 0.267 2 47 18 -0.247 2 48 48 0.707 2 49 10 0.000 2 50 11 0.226 2 51 12 -0.231 2 52 8 0.177 2 53 9 0.517 2 54 16 0.375 2 55 4 0.400 2 56 10 0.000 2 57 12 0.217 2 58 11 0.407 2 59 10 -0.142 2 60 15 0.478 2 61 22 -0.048 2 62 17 -0.049 2 63 12 -0.794 2 64 6 -0.102 2 65 4 -0.250 2 66 16 -0.250 2 67 10 0.000 2 68 20 0.101 2 69 33 -0.313 2 70 9 0.000 2 71 38 0.300 2 72 16 0.250 2 73 4 -0.250 2 74 35 0.127 2 75 4 -0.250 2 76 31 0.099 2 77 4 0.000 2 78 8 0.371 2 79 4 0.000 2 80 16 0.400 2 81 37 -0.033 2 82 23 0.000 2 83 15 0.000 2 84 14 0.294 2 85 26 0.000 2 86 8 0.265 2 87 17 0.194 2 88 9 0.067 2 89 18 0.118 2 90 37 0.205 2 91 15 0.000 2 92 23 0.000 2 93 28 -0.180 2 94 34 0.718 2 95 14 0.000 2 96 18 -0.141 2 97 19 0.184 2 98 26 0.147 2 99 35 0.355 2 100 19 -0.321 2 101 20 -0.369 2 102 22 -0.205 2 103 43 -0.290 2 104 23 -0.646 2 105 11 0.151 2 106 41 0.226 2 107 29 0.111 2 108 52 -0.125 2 109 30 0.256 2 110 11 0.090 2 111 20 -0.056 2 112 34 0.475 2 113 11 -0.121 2 114 7 -0.189 2 115 31 -0.180 2 116 42 0.548 2 117 5 0.000 2 118 41 0.372 2 119 48 0.217 2 120 13 0.485 2 121 52 0.582 2 122 13 0.607 2 123 15 0.129 2 124 7 -0.189 2 125 8 0.088 2 126 26 0.343 2 127 23 -0.021 2 128 9 0.033 2 129 11 0.151 2 130 7 0.000 3 1 11 0.000 3 2 24 -0.704 3 3 10 -0.791 3 4 39 0.192 3 5 35 -0.279 3 6 11 0.407 3 7 38 0.260 3 8 3 0.000 3 9 15 0.194 3 10 21 0.262 3 11 50 0.219 3 12 6 0.163 3 13 48 -0.051 3 14 4 0.125 3 15 5 0.000 3 16 13 0.277 3 17 26 0.000 3 18 14 0.134 3 19 48 -0.260 3 20 15 0.000 3 21 28 -0.104 3 22 21 -0.098 3 23 23 0.349 3 24 4 0.000 3 25 55 0.034 3 26 8 0.530 3 27 20 0.224 3 28 18 -0.795 3 29 13 0.000 3 30 113 0.461 3 31 17 -0.121 3 32 15 0.349 3 33 14 0.000 3 34 37 0.041 3 35 13 0.839 3 36 13 0.139 3 37 21 0.055 3 38 12 0.231 3 39 11 0.814 3 40 12 -0.029 3 41 11 0.030 3 42 29 0.474 3 43 29 -0.078 3 44 12 0.260 3 45 17 0.218 3 46 15 -0.181 3 47 34 0.223 3 48 18 0.118 3 49 23 0.083 3 50 27 -0.500 3 51 20 -0.447 3 52 27 0.250 3 53 10 0.158 3 54 11 -0.404 3 55 29 -0.046 3 56 25 0.040 3 57 18 0.318 3 58 59 0.358 3 59 28 0.028 3 60 5 0.000 3 61 17 0.182 3 62 25 0.420 3 63 10 0.079 3 64 11 -0.965 3 65 18 0.236 3 66 29 0.251 3 67 20 -0.123 3 68 22 0.320 3 69 12 0.375 3 70 23 0.261 3 71 12 0.000 3 72 13 -0.180 3 73 33 -0.566 3 74 13 -0.555 3 75 13 0.000 3 76 26 0.078 3 77 13 0.000 3 78 14 0.067 3 79 29 0.046 3 80 23 0.188 3 81 4 0.300 3 82 19 0.092 3 83 20 0.089 3 84 21 0.109 3 85 29 -0.390 3 86 7 -0.151 3 87 25 0.150 3 88 17 0.303 3 89 18 0.306 3 90 19 0.241 3 91 20 0.201 3 92 10 0.000 3 93 23 -0.188 3 94 24 0.082 3 95 11 -0.121 3 96 15 0.000 3 97 10 -0.285 3 98 16 0.125 3 99 45 0.447 3 100 7 0.000 3 101 17 0.206 3 102 20 0.531 3 103 12 0.173 3 104 12 0.000 3 105 11 0.226 3 106 43 0.221 3 107 19 -0.206 3 108 28 0.227 3 109 26 0.029 3 110 25 0.160 3 111 9 0.000 3 112 12 0.000 3 113 7 0.189 3 114 3 0.462 3 115 13 -0.555 3 116 38 -0.032 3 117 33 -0.374 3 118 12 -0.217 3 119 11 0.377 3 120 29 0.511 3 121 33 0.174 3 122 5 -0.112 3 123 9 0.083 3 124 10 0.000 3 125 19 0.103 3 126 23 0.302 3 127 16 -0.125 3 128 16 0.000 3 129 10 0.000 3 130 9 -0.100 3 131 19 0.057 3 132 15 -0.161 3 133 11 0.226 3 134 30 -0.128 3 135 8 0.530 3 136 23 0.480 3 137 28 -0.333 3 138 41 0.078 3 139 12 0.260 3 140 19 0.258 3 141 11 0.226 3 142 13 0.347 3 143 7 0.189 3 144 24 -0.082 3 145 19 0.379 3 146 28 0.151 3 147 18 0.997 3 148 29 -0.416 3 149 21 -0.371 3 150 22 0.341 3 151 16 -0.100 3 152 26 0.353 3 153 19 0.578 3 154 15 0.000 3 155 24 -0.153 3 156 9 0.027 3 157 10 0.000 3 158 14 -0.160 3 159 32 -0.071 3 160 14 -0.274 3 161 24 -0.122 3 162 6 0.000 3 163 12 -0.289 3 164 16 0.000 3 165 1 0.000 3 166 16 0.150 3 167 26 0.392 3 168 10 0.126 3 169 23 0.209 3 170 8 0.460 3 171 4 0.000 3 172 27 0.308 3 173 12 0.577 3 174 4 0.000 3 175 4 0.000 3 176 3 0.000 macron.senti&lt;-as_tibble(macron.senti) macron.senti element_id sentence_id word_count sentiment 1 1 34 -0.103 1 2 20 0.034 1 3 24 -0.131 1 4 41 -1.325 1 5 12 0.072 1 6 27 0.000 1 7 29 -0.371 1 8 19 0.115 1 9 52 0.653 1 10 10 0.395 1 11 24 -0.020 1 12 12 -0.447 1 13 24 0.388 1 14 7 0.189 1 15 60 0.400 1 16 7 0.000 1 17 10 0.111 1 18 8 -0.354 1 19 42 0.108 1 20 5 0.000 1 21 12 0.361 1 22 15 0.129 1 23 37 0.329 1 24 26 -0.098 1 25 8 -0.265 1 26 15 -0.426 1 27 11 0.754 1 28 37 0.214 1 29 34 -0.054 1 30 19 -0.133 1 31 37 -0.378 1 32 4 0.000 1 33 16 -0.125 1 34 8 0.230 1 35 31 -0.404 1 36 29 0.183 1 37 21 0.000 1 38 17 -0.424 1 39 26 0.255 1 40 17 0.094 1 41 16 0.250 1 42 17 0.061 1 43 34 0.120 1 44 28 0.546 1 45 18 0.377 1 46 8 -0.141 1 47 1 0.000 1 48 32 -0.194 1 49 36 0.108 1 50 61 0.045 1 51 17 0.061 1 52 49 0.507 1 53 21 -0.055 1 54 13 0.139 1 55 15 0.026 1 56 9 0.000 1 57 18 0.012 1 58 22 -0.128 1 59 13 -0.208 1 60 26 0.475 1 61 8 0.194 1 62 18 0.236 1 63 8 0.141 1 64 8 0.354 1 65 6 0.000 1 66 6 0.408 1 67 64 0.325 1 68 27 0.327 1 69 6 0.000 1 70 21 0.404 1 71 64 0.388 1 72 9 0.000 1 73 22 0.927 1 74 20 0.637 1 75 21 0.109 1 76 4 0.300 1 77 26 0.284 1 78 16 0.462 1 79 9 0.000 1 80 13 0.413 1 81 15 0.103 1 82 15 -0.697 1 83 33 -0.087 1 84 14 -0.267 1 85 18 -0.094 1 86 19 -0.344 1 87 7 0.076 1 88 16 -0.088 1 89 30 -0.037 1 90 19 0.344 1 91 15 0.207 1 92 8 0.000 1 93 7 0.000 1 94 10 -0.126 1 95 23 0.849 1 96 36 0.133 1 97 12 0.217 1 98 14 0.347 1 99 28 0.283 1 100 25 -0.550 1 101 11 0.075 1 102 20 0.000 1 103 10 0.237 1 104 7 0.283 1 105 29 0.121 1 106 7 0.302 1 107 10 0.253 1 108 4 -0.125 1 109 5 0.000 1 110 11 0.000 1 111 23 0.344 1 112 6 -0.163 1 113 16 0.125 1 114 17 0.158 1 115 9 0.400 1 116 24 0.122 1 117 18 0.460 1 118 24 0.000 1 119 23 0.083 1 120 32 0.256 1 121 20 0.291 1 122 16 0.162 1 123 20 -0.078 1 124 12 0.000 1 125 30 0.228 1 126 18 0.059 1 127 15 0.129 1 128 7 -0.189 1 129 10 0.411 1 130 7 0.397 1 131 10 0.569 1 132 20 0.615 1 133 8 0.000 1 134 12 0.115 1 135 45 0.861 1 136 14 -0.748 1 137 9 0.133 1 138 24 -0.214 1 139 23 0.000 1 140 19 -0.252 1 141 6 0.000 1 142 25 -0.180 1 143 6 -0.408 1 144 13 -0.347 1 145 17 -0.121 1 146 9 0.000 1 147 7 -0.283 1 148 3 0.289 1 149 13 0.277 1 150 14 0.347 1 151 17 0.024 1 152 5 0.179 1 153 11 0.166 1 154 5 0.000 1 155 12 -0.014 1 156 39 0.328 1 157 17 0.061 1 158 15 0.491 1 159 18 0.059 1 160 14 0.388 1 161 7 0.189 1 162 8 0.000 1 163 19 0.344 1 164 16 0.000 1 165 15 -0.129 1 166 24 0.000 1 167 34 0.214 1 168 7 0.000 1 169 27 0.241 1 170 16 0.125 1 171 29 0.093 1 172 22 0.426 1 173 44 0.030 1 174 36 -0.067 1 175 22 -0.085 1 176 19 0.619 1 177 33 0.104 1 178 12 0.000 1 179 6 0.000 1 180 16 0.300 1 181 40 0.032 1 182 15 0.194 1 183 5 0.000 1 184 4 0.000 1 185 3 0.000 2 1 21 -0.164 2 2 23 -0.156 2 3 16 -0.188 2 4 12 0.173 2 5 22 -0.128 2 6 7 -0.302 2 7 17 0.121 2 8 26 0.559 2 9 35 0.355 2 10 7 0.302 2 11 32 0.318 2 12 34 0.086 2 13 21 0.218 2 14 13 0.111 2 15 42 -0.085 2 16 7 0.000 2 17 50 0.404 2 18 11 -0.030 2 19 26 0.029 2 20 24 0.245 2 21 40 0.245 2 22 18 0.000 2 23 41 0.472 2 24 9 0.027 2 25 43 -0.380 2 26 9 0.167 2 27 23 0.031 2 28 10 0.111 2 29 38 0.008 2 30 14 0.000 2 31 18 0.000 2 32 17 0.000 2 33 12 0.043 2 34 75 0.237 2 35 43 0.244 2 36 17 0.255 2 37 10 -0.079 2 38 38 0.105 2 39 6 0.551 2 40 18 0.071 2 41 14 0.167 2 42 12 0.289 2 43 34 -0.026 2 44 15 0.090 2 45 6 0.367 2 46 14 0.267 2 47 18 -0.247 2 48 48 0.707 2 49 10 0.000 2 50 11 0.226 2 51 12 -0.231 2 52 8 0.177 2 53 9 0.517 2 54 16 0.375 2 55 4 0.400 2 56 10 0.000 2 57 12 0.217 2 58 11 0.407 2 59 10 -0.142 2 60 15 0.478 2 61 22 -0.048 2 62 17 -0.049 2 63 12 -0.794 2 64 6 -0.102 2 65 4 -0.250 2 66 16 -0.250 2 67 10 0.000 2 68 20 0.101 2 69 33 -0.313 2 70 9 0.000 2 71 38 0.300 2 72 16 0.250 2 73 4 -0.250 2 74 35 0.127 2 75 4 -0.250 2 76 31 0.099 2 77 4 0.000 2 78 8 0.371 2 79 4 0.000 2 80 16 0.400 2 81 37 -0.033 2 82 23 0.000 2 83 15 0.000 2 84 14 0.294 2 85 26 0.000 2 86 8 0.265 2 87 17 0.194 2 88 9 0.067 2 89 18 0.118 2 90 37 0.205 2 91 15 0.000 2 92 23 0.000 2 93 28 -0.180 2 94 34 0.718 2 95 14 0.000 2 96 18 -0.141 2 97 19 0.184 2 98 26 0.147 2 99 35 0.355 2 100 19 -0.321 2 101 20 -0.369 2 102 22 -0.205 2 103 43 -0.290 2 104 23 -0.646 2 105 11 0.151 2 106 41 0.226 2 107 29 0.111 2 108 52 -0.125 2 109 30 0.256 2 110 11 0.090 2 111 20 -0.056 2 112 34 0.475 2 113 11 -0.121 2 114 7 -0.189 2 115 31 -0.180 2 116 42 0.548 2 117 5 0.000 2 118 41 0.372 2 119 48 0.217 2 120 13 0.485 2 121 52 0.582 2 122 13 0.607 2 123 15 0.129 2 124 7 -0.189 2 125 8 0.088 2 126 26 0.343 2 127 23 -0.021 2 128 9 0.033 2 129 11 0.151 2 130 7 0.000 3 1 11 0.000 3 2 24 -0.704 3 3 10 -0.791 3 4 39 0.192 3 5 35 -0.279 3 6 11 0.407 3 7 38 0.260 3 8 3 0.000 3 9 15 0.194 3 10 21 0.262 3 11 50 0.219 3 12 6 0.163 3 13 48 -0.051 3 14 4 0.125 3 15 5 0.000 3 16 13 0.277 3 17 26 0.824 3 18 14 0.134 3 19 48 -0.260 3 20 15 0.000 3 21 28 -0.104 3 22 21 -0.098 3 23 23 0.349 3 24 4 0.000 3 25 55 0.034 3 26 8 0.530 3 27 20 0.224 3 28 18 -0.795 3 29 13 0.000 3 30 113 0.461 3 31 17 -0.121 3 32 15 0.349 3 33 14 0.000 3 34 37 0.041 3 35 13 0.839 3 36 13 0.139 3 37 21 0.055 3 38 12 0.231 3 39 11 0.814 3 40 12 -0.029 3 41 11 0.030 3 42 29 0.474 3 43 29 -0.078 3 44 12 0.260 3 45 17 0.218 3 46 15 -0.181 3 47 34 0.223 3 48 18 0.118 3 49 23 0.083 3 50 27 -0.500 3 51 20 -0.447 3 52 27 0.250 3 53 10 0.158 3 54 11 -0.404 3 55 29 -0.046 3 56 25 0.040 3 57 18 0.318 3 58 59 0.358 3 59 28 0.028 3 60 5 0.000 3 61 17 0.182 3 62 25 0.420 3 63 10 0.079 3 64 11 -0.965 3 65 18 0.236 3 66 29 0.251 3 67 20 -0.123 3 68 22 0.320 3 69 12 0.375 3 70 23 0.261 3 71 12 0.000 3 72 13 -0.180 3 73 33 -0.566 3 74 13 -0.555 3 75 13 0.000 3 76 26 0.078 3 77 13 0.000 3 78 14 0.067 3 79 29 0.046 3 80 23 0.188 3 81 4 0.300 3 82 19 0.092 3 83 20 0.089 3 84 21 0.109 3 85 29 -0.390 3 86 7 -0.151 3 87 25 0.150 3 88 17 0.303 3 89 18 0.306 3 90 19 0.241 3 91 20 0.201 3 92 10 0.000 3 93 23 -0.188 3 94 24 0.082 3 95 11 -0.121 3 96 15 0.000 3 97 10 -0.285 3 98 16 0.125 3 99 45 0.447 3 100 7 0.000 3 101 17 0.206 3 102 20 0.531 3 103 12 0.173 3 104 12 0.000 3 105 11 0.226 3 106 43 0.221 3 107 19 -0.206 3 108 28 0.227 3 109 26 0.029 3 110 25 0.160 3 111 9 -0.117 3 112 12 0.000 3 113 7 0.189 3 114 3 0.462 3 115 13 -0.555 3 116 38 -0.032 3 117 33 -0.374 3 118 12 -0.217 3 119 11 0.377 3 120 29 0.511 3 121 33 0.174 3 122 5 -0.112 3 123 9 0.083 3 124 10 0.000 3 125 19 0.103 3 126 23 0.302 3 127 16 -0.125 3 128 16 0.000 3 129 10 0.000 3 130 9 -0.100 3 131 19 0.057 3 132 15 -0.161 3 133 11 0.226 3 134 30 -0.128 3 135 8 0.530 3 136 23 0.480 3 137 28 -0.333 3 138 41 0.078 3 139 12 0.260 3 140 19 0.258 3 141 11 0.226 3 142 13 0.347 3 143 7 0.189 3 144 24 -0.082 3 145 19 0.379 3 146 28 0.151 3 147 18 0.997 3 148 29 -0.416 3 149 21 -0.371 3 150 22 0.341 3 151 16 -0.100 3 152 26 0.353 3 153 19 0.578 3 154 15 0.000 3 155 24 -0.153 3 156 9 0.027 3 157 10 0.000 3 158 14 -0.160 3 159 32 -0.071 3 160 14 -0.274 3 161 24 -0.122 3 162 6 0.000 3 163 12 -0.289 3 164 16 0.000 3 165 1 0.000 3 166 16 0.150 3 167 26 0.392 3 168 10 0.126 3 169 23 0.209 3 170 8 0.460 3 171 4 0.000 3 172 27 0.308 3 173 12 0.577 3 174 4 0.000 3 175 4 0.000 3 176 3 0.000 macron.senti%&gt;% group_by(element_id) %&gt;% ggplot(aes(x=sentence_id,y=sentiment, col = element_id )) + geom_line() + facet_wrap(~element_id) + ggtitle(&quot;Macron: Evolution of the sentiments \\nwithin speeches using valence shifters&quot;) + ylab(&quot;Document&quot;) + xlab(&quot;Sentences in speeches&quot;) Figure 4.4: Evolution of the sentiments within speeches using valence shifters "],["similarities.html", "Chapter 5 Similarities 5.1 Boris 5.2 Macron 5.3 Comparison", " Chapter 5 Similarities library(readr) library(quanteda) library(knitr) library(kableExtra) library(reshape2) library(ggplot2) The aim of this part of the project is to compute the similarities and dissimilarities between the different doscuments for both Johnson and Macron. We will use here the previously cleaned and tokenised corpuses and the two TF-IDF matrices computed when performing the exploratory data analysis. The three metrics used are the following; the Jaccard Similarity (similarity measure), the Cosine Similarity (similarity measure) and the Euclidean Distance (dissimilarity measure, bounded by the largest distance that is present in the corpus, can therefore be rescaled to a similarity measure between 0 and 1, 1 being the largest distance in the corpus). In order to get a better visualisation of the three metrics, we used a heatmap representation (similarity = 0 --&gt; yellow and similarity = 1 --&gt; red). Actually, when looking at the various heatmaps drawn when running the code, all those similarity measures show the same results, there is not any large similarity between the different documents for Boris Johnson. The only cases on the heatmap that are red are the ones that are on the diagonal, which corresponds to the similarity of a given document and itself, which is equal to 1. 5.1 Boris ## Jaccard Similarity boris.jac &lt;- textstat_simil(corpus_boris.tfidf, method = &quot;jaccard&quot;, margin = &quot;documents&quot;) ## Cosine Similarity boris.cos &lt;- textstat_simil(corpus_boris.tfidf, method = &quot;cosine&quot;, margin = &quot;documents&quot;) ## Euclidean Distance boris.euc &lt;- textstat_dist(corpus_boris.tfidf, method = &quot;euclidean&quot;, margin = &quot;documents&quot;) ## Jaccard Matrix boris.jac.mat &lt;- melt(as.matrix(boris.jac)) ggplot(data=boris.jac.mat, aes(x=Var1, y=Var2, fill=value)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint =0.5, limit=c(0,1), name=&quot;Jaccard&quot;) + geom_tile() ## Cosine Matrix boris.cos.mat &lt;- melt(as.matrix(boris.cos)) ggplot(data=boris.cos.mat, aes(x=Var1, y=Var2, fill=value)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint=0.5, limit=c(0,1), name=&quot;Cosine&quot;) + geom_tile() ## Euclidean Matrix boris.euc.mat &lt;- melt(as.matrix(boris.euc)) M &lt;- max(boris.euc.mat$value) boris.euc.mat$value.std &lt;- (M-boris.euc.mat$value)/M ggplot(data=boris.euc.mat, aes(x=Var1, y=Var2, fill=boris.euc.mat$value.std)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint=0.5, limit=c(0,1),name =&quot;Euclidean&quot;) + geom_tile() We then used two different clustering methods, hierarchical clustering (dendrogram) and partitioning (K-means method). We see that the results are quite similar. When looking at the 10 most common words per cluster, there are some words that appear when using the first method and the second one. ## Clustering ## Jaccard Method boris.hc &lt;- hclust(dist(boris.euc)) boris.hc &lt;- hclust(dist(1 - boris.jac)) plot(boris.hc) ## Cosine Method boris.hc &lt;- hclust(dist(boris.euc)) boris.hc &lt;- hclust(dist(1 - boris.cos)) plot(boris.hc) ## Dendrogram = Hierarchical Clustering boris.clust &lt;- cutree(boris.hc, k=3) boris.clust #&gt; text1 text2 text3 text4 text5 text6 text7 #&gt; 1 2 2 2 3 3 3 ## K-means Method = Partitionning boris.km &lt;- kmeans(corpus_boris.tfidf, centers=3) boris.km$cluster #&gt; text1 text2 text3 text4 text5 text6 text7 #&gt; 2 2 1 3 2 2 2 ### Extracting the 10 most used words - Dendrogram data.frame( clust1 = names(sort(apply(corpus_boris.tfidf[boris.clust==1,],2,sum), decreasing = TRUE)[1:10]), clust2 = names(sort(apply(corpus_boris.tfidf[boris.clust==2,],2,sum), decreasing = TRUE)[1:10]), clust3 = names(sort(apply(corpus_boris.tfidf[boris.clust==3,],2,sum), decreasing = TRUE)[1:10]) ) clust1 clust2 clust3 outbreak school already chris child see four mass progress manage parent robert tackle period jenrick minister public behind phase ensure thousand thing important bite patrick dangerous virus delay london huge ### Extracting the 10 most used words - K-Means data.frame( clust1 = names(sort(apply(corpus_boris.tfidf[boris.km$cluster==1,],2,sum), decreasing = TRUE)[1:10]), clust2 = names(sort(apply(corpus_boris.tfidf[boris.km$cluster==2,],2,sum), decreasing = TRUE)[1:10]), clust3 = names(sort(apply(corpus_boris.tfidf[boris.km$cluster==3,],2,sum), decreasing = TRUE)[1:10]) ) clust1 clust2 clust3 mass outbreak school london already child contact see parent ensure chris pupil gathering dangerous teacher fight progress already without robert fightback stop jenrick update non-essential virus judgment rather public downward When computing document similarities for Macron, we also observe that the only elements on the heatmap that are represented by the red colour are situated on the digonal. However, we can notice a slight difference here compared to Johnson. We do observe that there is a small similarity between document 1 and document 2 for Macron. This is to say that he used the same tokens both in the first and the second document. 5.2 Macron ## Jaccard Similarity macron.jac &lt;- textstat_simil(corpus_macron.tfidf, method = &quot;jaccard&quot;, margin = &quot;documents&quot;) ## Cosine Similarity macron.cos &lt;- textstat_simil(corpus_macron.tfidf, method = &quot;cosine&quot;, margin = &quot;documents&quot;) ## Euclidean Distance macron.euc &lt;- textstat_dist(corpus_macron.tfidf, method = &quot;euclidean&quot;, margin = &quot;documents&quot;) ## Jaccard Matrix macron.jac.mat &lt;- melt(as.matrix(macron.jac)) ggplot(data=macron.jac.mat, aes(x=Var1, y=Var2, fill=value)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint =0.5, limit=c(0,1), name=&quot;Jaccard&quot;) + geom_tile() ## Cosine Matrix macron.cos.mat &lt;- melt(as.matrix(macron.cos)) ggplot(data=macron.cos.mat, aes(x=Var1, y=Var2, fill=value)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint=0.5, limit=c(0,1), name=&quot;Cosine&quot;) + geom_tile() ## Euclidean Matrix macron.euc.mat &lt;- melt(as.matrix(macron.euc)) M &lt;- max(macron.euc.mat$value) macron.euc.mat$value.std &lt;- (M-macron.euc.mat$value)/M ggplot(data=macron.euc.mat, aes(x=Var1, y=Var2, fill=macron.euc.mat$value.std)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint=0.5, limit=c(0,1),name =&quot;Euclidean&quot;) + geom_tile() 5.3 Comparison "],["topic-modelling.html", "Chapter 6 Topic Modelling 6.1 Boris Johnson 6.2 Macron", " Chapter 6 Topic Modelling In this chapter, we analzye the topics of the speechs of Boris Jonhson and Macron using : LSA(Latent Semantic analysis). The core idea is to take a matrix of what we have ‚Äî documents and terms ‚Äî and decompose it into a separate document-topic matrix and a topic-term matrix. LDA(Latent Dirichlet Allocation).It uses dirichlet priors for the document-topic and word-topic distributions, lending itself to better generalization. And then we will combine the two dataset and do the same analysis. 6.1 Boris Johnson 6.1.1 LSA First, we make the DTM matrix. We are goin to use 3 dimensions, it means 3 differents topics. bmod&lt;-textmodel_lsa(corpus_boris.dfm,nd=3) To inspect the results, we can extract the matrices involved in the LSA decomposition. In the firs table, each components measures the link between the document and the topic. In the second table, each component measure the link between the document and the term. LSA is typical a reduction technique. Insetead of have N documents or M term, it is represented by K documents. lsa_docs_boris&lt;-head(bmod$docs) lsa_docs_boris&lt;-data.frame(lsa_docs_boris) lsa_docs_boris%&gt;% kable(caption=&quot; Link between document and topic&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 6.1: Link between document and topic X1 X2 X3 text1 -0.175 0.053 -0.148 text2 -0.519 0.052 -0.813 text3 -0.358 -0.737 0.101 text4 -0.525 0.626 0.342 text5 -0.299 -0.210 0.314 text6 -0.356 -0.121 0.226 head(bmod$features) morning -0.002 0.002 -0.005 government's -0.011 0.005 -0.039 cobr -0.007 -0.023 -0.002 emergency -0.030 -0.050 -0.011 committee -0.009 0.004 -0.034 coronavirus -0.047 0.015 -0.026 lsa_features_boris&lt;-head(bmod$features) lsa_features_boris&lt;-data.frame(lsa_features_boris) lsa_features_boris%&gt;% kable(caption=&quot; Link between document and terms&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 6.2: Link between document and terms X1 X2 X3 morning -0.002 0.002 -0.005 government's -0.011 0.005 -0.039 cobr -0.007 -0.023 -0.002 emergency -0.030 -0.050 -0.011 committee -0.009 0.004 -0.034 coronavirus -0.047 0.015 -0.026 Often the first dimension in LSA is associated with the document lenght. To see if it is true, we build a scatter-plot between the document lengt and Dimension 1. As we observe in the figure 6.1, the dimension 1 is negatively correlated with the document lenght. Therefore the dimension 1 bring us not a lot of informations that we have already. ns&lt;-apply(corpus_boris.dfm,1,sum) plot(ns~bmod$docs [,1]) Figure 6.1: First dimension of the LSA - Boris Johnson We clearly observe that the dimension 1 is negatively correlated with the document lenght. Now in order to make the link between the topics and the documents and the topics with term, we use biplot. We represent the dimension 2 and 3, beacause often the first component bring often little information. Reminders: The seven speech are class by chronological order: * 09 March (text1) * 12 March (text2) * 16 March (text3) * 18 March (text5) * 19 March (text6) * 20 March (text7) * 22 March (text8) It is noticeable that the texts that are brought together over time are grouped together.And that the first speeches are the opposite of the last ones as we observe in the figure 6.2 biplot(y=bmod$docs[,2:3],x=bmod$features[,2:3], col=c(&quot;grey&quot;,&quot;red&quot;), xlab = &quot;Dimension 2&quot;, ylab=&quot;Dimension 3&quot;) Figure 6.2: Biplot - Boris Johnson We repeat the same analysis with TF-IDF. The influence of small weighted-frequent tokens is reduced. bmod_2&lt;- textmodel_lsa(corpus_boris.tfidf, nd=3) head(bmod_2$docs) text1 -0.147 -0.063 -0.318 text2 -0.380 -0.187 -0.752 text3 -0.444 -0.750 0.463 text4 -0.735 0.615 0.256 text5 -0.180 -0.132 -0.173 text6 -0.200 -0.040 -0.153 head(bmod_2$features) morning -0.008 -0.004 -0.025 government's -0.024 -0.014 -0.070 cobr -0.021 -0.037 0.007 emergency -0.042 -0.038 0.013 committee -0.019 -0.011 -0.054 coronavirus -0.016 -0.001 -0.011 6.1.2 LDA We now turn to the LDA. For illustration, we will make K=3 topis. K&lt;-3 corpus_boris.dtm&lt;- convert(corpus_boris.dfm, to=&quot;topicmodels&quot;) lda_boris&lt;- LDA(corpus_boris.dtm ,k=K) In the table 6.3, it is the list of the six most frequent term in each topic terms&lt;-terms(lda_boris,6) terms&lt;-data.frame(terms) terms %&gt;% kable(caption=&quot;List of the terms present in each topic&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 6.3: List of the terms present in each topic Topic.1 Topic.2 Topic.3 will will will now much want need can go go now thank make people can school disease day In the table 6.4, you can observe which text is related to which topic. ## To see the topics related to each document topics&lt;-(topics(lda_boris,1)) topics&lt;-data.frame(topics) topics%&gt;% kable(caption=&quot;Topics&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 6.4: Topics topics text1 2 text2 2 text3 1 text4 1 text5 3 text6 2 text7 3 We now build the bar plot to inspect the per-topic-per-word probabilities (beta's). We take the 10 top terms and rearrange the beta per topic according to this order. We observe in the figure 6.3 that the topic 1 that the 2 first terms are &quot;now&quot; and &quot;go&quot;. We can image that the topic 1 is more focous on the urgency and on the keep going. beta.td.boris&lt;-tidy(lda_boris,matrix=&quot;beta&quot;) beta.top.term.boris&lt;-beta.td.boris %&gt;% group_by(topic) %&gt;% top_n(10, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) beta.top.term.boris %&gt;% mutate(term=reorder_within(term, beta, topic)) %&gt;% ggplot(aes(term,beta,fill=factor(topic))) + geom_col(show.legend = FALSE)+ facet_wrap(~topic, scales = &quot;free&quot;) + coord_flip()+ scale_x_reordered() Figure 6.3: Beta - Boris Johnson Now, we compute the gamma, it shows the proportion of each topic within each document, as you can observe in the figure 6.4. We note that each document represented a text.The texts are very distinctive. gamma.td.boris&lt;- tidy(lda_boris,matrix=&quot;gamma&quot;) gamma.td.boris %&gt;% ggplot(aes(document,gamma,fill=factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~topic,scales = &quot;free&quot;)+ coord_flip()+ scale_x_reordered() Figure 6.4: Gamma - Boris Johnson 6.2 Macron 6.2.1 LSA mmod&lt;-textmodel_lsa(corpus_macron.dfm,nd=3) To inspect the results, we can extract the matrices involved in the LSA decomposition head(mmod$docs) text1 -0.666 0.502 0.552 text2 -0.426 0.351 -0.834 text3 -0.612 -0.791 -0.020 head(mmod$features) france -0.080 0.032 0.009 dear -0.054 0.016 -0.022 past -0.023 -0.017 0.037 country -0.075 -0.107 -0.071 spread -0.049 0.077 0.024 virus -0.101 -0.015 0.085 Often the first dimension in LSA is associated with the document lenght. To see if it is true, we build a scatter-plot between the document lengt and Dimension 1. ns_macron&lt;-apply(corpus_macron.dfm,1,sum) plot(ns_macron~mmod$docs [,1]) We clearly observe that the dimension 1 is negatively correlated with the document lenght. Now in order to make the link between the topics and the documents and the topics with term, we use biplot biplot(y=mmod$docs[,2:3],x=mmod$features[,2:3], col=c(&quot;grey&quot;,&quot;red&quot;), xlab = &quot;Dimension 2&quot;, ylab=&quot;Dimension 3&quot;) We repeat the same analysis with TF-IDF mmod_2&lt;- textmodel_lsa(corpus_macron.tfidf, nd=3) head(mmod_2$docs) text1 -0.244 0.921 0.304 text2 -0.107 0.286 -0.952 text3 -0.964 -0.265 0.028 head(mmod_2$features) france 0.000 0.000 0.000 dear 0.000 0.000 0.000 past -0.042 0.028 0.016 country 0.000 0.000 0.000 spread 0.000 0.000 0.000 virus 0.000 0.000 0.000 6.2.2 LDA We now turn to the LDA. For illustration, we will make K=5 topis. K&lt;-5 corpus_macron.dtm&lt;- convert(corpus_macron.dfm, to=&quot;topicmodels&quot;) lda_macron&lt;- LDA(corpus_macron.dtm ,k=K) Top terms per topic and top topic per document can be easily obtained. Belo, the six top terms and the top topic are extracted. terms(lda_macron,6) Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 will will will will will work go take also much month way also must can much even much much continue many one even day must crisis ask health protect week topics(lda_macron,1) ## To see the topics related to each document #&gt; text1 text2 text3 #&gt; 3 4 5 We now build the bar plot to inspect the per-topic-per-word probabilities (beta's). We take the 10 top terms and rearrange the beta per topic according to this order. beta.td.macron&lt;-tidy(lda_macron,matrix=&quot;beta&quot;) beta.top.term.macron&lt;-beta.td.macron %&gt;% group_by(topic) %&gt;% top_n(10, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) beta.top.term.macron %&gt;% mutate(term=reorder_within(term, beta, topic)) %&gt;% ggplot(aes(term,beta,fill=factor(topic))) + geom_col(show.legend = FALSE)+ facet_wrap(~topic, scales = &quot;free&quot;) + coord_flip()+ scale_x_reordered() Now, we compute the gamma, it shows the proportion of each topic within each document. We note that text 1 is related to the topic 1 4 and 5. Th test 3 is related to the topic 2. And the text 2 is related to the topic 3. gamma.td.macron&lt;- tidy(lda_macron,matrix=&quot;gamma&quot;) gamma.td.macron %&gt;% ggplot(aes(document,gamma,fill=factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~topic,scales = &quot;free&quot;)+ coord_flip()+ scale_x_reordered() "],["word-embedding.html", "Chapter 7 Word Embedding 7.1 Boris Johnson 7.2 Macron 7.3 Comparison", " Chapter 7 Word Embedding 7.1 Boris Johnson Here, we compute the co-occurence matrix. We use the fcm function from quanteda. We use a window lenght 5. speech.coo.boris&lt;-fcm(corpus_boris,context=&quot;window&quot;,window=5, tri=FALSE) p&lt;-2 #word embedding dimension speech.glove.boris&lt;-GlobalVectors$new(rank = p,x_max = 10) #xmas is a neede technical option speech.weC.boris&lt;-speech.glove.boris$fit_transform(speech.coo.boris) #&gt; INFO [19:01:31.632] epoch 1, loss 0.0346 #&gt; INFO [19:01:31.784] epoch 2, loss 0.0245 #&gt; INFO [19:01:31.826] epoch 3, loss 0.0225 #&gt; INFO [19:01:31.833] epoch 4, loss 0.0215 #&gt; INFO [19:01:31.843] epoch 5, loss 0.0207 #&gt; INFO [19:01:31.854] epoch 6, loss 0.0201 #&gt; INFO [19:01:31.864] epoch 7, loss 0.0195 #&gt; INFO [19:01:31.873] epoch 8, loss 0.0189 #&gt; INFO [19:01:31.887] epoch 9, loss 0.0183 #&gt; INFO [19:01:31.896] epoch 10, loss 0.0177 For illustration purpose, we now plot the 50 most used terms n.w.boris&lt;-apply(corpus_boris.dfm,2,sum) #compute the number of times each term is used index&lt;-order(n.w.boris,decreasing = TRUE)[1:50] plot(speech.weC.boris[index,],type = &quot;n&quot;,xlab = &quot;Dimension 1&quot;, ylab = &quot;Dimendion 2&quot;) text(x=speech.weC.boris[index,],labels = rownames(speech.weC.boris[index,])) speech.dtm &lt;- corpus_boris.dfm speech.rwmd.model.boris&lt;-RelaxedWordMoversDistance$new(corpus_boris.dfm,speech.weC.boris) speech.rwms.boris&lt;-speech.rwmd.model.boris$sim2(corpus_boris.dfm) speech.rwmd.boris&lt;-speech.rwmd.model.boris$dist2(corpus_boris.dfm) speech.hc.boris&lt;-hclust(as.dist(speech.rwmd.boris)) plot(speech.hc.boris,cex=0.8) We can observe that there is some coherence within the groups in terms the date of the speech. speech.cl.boris&lt;- cutree(speech.hc.boris,k=4) corpus_boris.dfm[speech.cl.boris==1,] #&gt; Document-feature matrix of: 3 documents, 797 features (69.4% sparse). #&gt; features #&gt; docs morning government&#39;s cobr emergency committee coronavirus #&gt; text1 1 2 1 1 1 3 #&gt; text2 0 1 0 1 1 2 #&gt; text4 0 0 0 1 0 2 #&gt; features #&gt; docs outbreak first scotland minister #&gt; text1 5 4 1 3 #&gt; text2 0 0 1 1 #&gt; text4 0 0 0 0 #&gt; [ reached max_nfeat ... 787 more features ] 7.2 Macron speech.coo.macron&lt;-fcm(corpus_macron,context=&quot;window&quot;,window=5, tri=FALSE) p&lt;-2 #word embedding dimension speech.glove.macron&lt;-GlobalVectors$new(rank = p,x_max = 10) #xmas is a neede technical option speech.weC.macron&lt;-speech.glove.macron$fit_transform(speech.coo.macron) #&gt; INFO [19:01:36.821] epoch 1, loss 0.0239 #&gt; INFO [19:01:36.840] epoch 2, loss 0.0176 #&gt; INFO [19:01:36.858] epoch 3, loss 0.0161 #&gt; INFO [19:01:36.876] epoch 4, loss 0.0153 #&gt; INFO [19:01:36.886] epoch 5, loss 0.0147 #&gt; INFO [19:01:36.899] epoch 6, loss 0.0142 #&gt; INFO [19:01:36.913] epoch 7, loss 0.0137 #&gt; INFO [19:01:36.923] epoch 8, loss 0.0131 #&gt; INFO [19:01:36.933] epoch 9, loss 0.0125 #&gt; INFO [19:01:36.943] epoch 10, loss 0.0119 For illustration purpose, we now plot the 50 most used terms n.w.macron&lt;-apply(corpus_macron.dfm,2,sum) #compute the number of times each term is used index&lt;-order(n.w.macron,decreasing = TRUE)[1:50] plot(speech.weC.macron[index,],type = &quot;n&quot;,xlab = &quot;Dimension 1&quot;, ylab = &quot;Dimendion 2&quot;) text(x=speech.weC.macron[index,],labels = rownames(speech.weC.macron[index,])) speech.dtm.macron &lt;- corpus_macron.dfm speech.rwmd.model.macron&lt;-RelaxedWordMoversDistance$new(corpus_macron.dfm,speech.weC.macron) speech.rwms.macron&lt;-speech.rwmd.model.macron$sim2(corpus_macron.dfm) speech.rwmd.macron&lt;-speech.rwmd.model.macron$dist2(corpus_macron.dfm) speech.hc.macron&lt;-hclust(as.dist(speech.rwmd.macron)) plot(speech.hc.macron,cex=0.8) We can observe that there is some coherence within the groups in terms the date of the speech. speech.cl.macron&lt;- cutree(speech.hc.macron,k=2) corpus_macron.dfm[speech.cl.macron==1,] #&gt; Document-feature matrix of: 2 documents, 1,469 features (47.4% sparse). #&gt; features #&gt; docs france dear past country spread virus covid-19 several #&gt; text1 10 6 3 4 8 13 4 5 #&gt; text3 7 5 3 12 2 11 1 5 #&gt; features #&gt; docs thousand fellow #&gt; text1 2 4 #&gt; text3 1 1 #&gt; [ reached max_nfeat ... 1,459 more features ] 7.3 Comparison "],["supervised-learning.html", "Chapter 8 Supervised learning 8.1 LSA 8.2 Random forest 8.3 Improving the features", " Chapter 8 Supervised learning In this section, we use supervised learning to develop a classifier of speech. The final aim is to be able to classify a speech from Boris Johnson or Emmanuel Macron. Therefore we are going to combine the dataframe of Boris with the dataframe of Macron. And because we don't have enough text, we are going to split the text into sentence in order to have enough data. ##Boris Johnson boris_2&lt;-as_tibble(c(boris9mars,boris12mars,boris16mars,boris18mars,boris19mars,boris20mars,boris22mars)) %&gt;% rename( text=value) author=&quot;Boris Johnson&quot; boris_supervised&lt;- cbind(boris_2, author) boris_2_sentence&lt;-get_sentences(boris_supervised) ##Emmanuel Macron Macron_2&lt;-as_tibble(c(macron12march,macron16march,macron13april)) %&gt;% rename( text = value) author=&quot;Macron&quot; macron_supervised&lt;- cbind(Macron_2, author) macron_2_sentence&lt;-get_sentences(macron_supervised) ##Combine the 2 dataframes combine &lt;- rbind(boris_2_sentence, macron_2_sentence) ## Tokenization combine_corpus&lt;-corpus(combine) combine_tokens&lt;- tokens(combine_corpus, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE) ##combi Lemmatization combine_tokens &lt;- tokens_replace(combine_tokens, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) ## Cleaning combine_tokens = combine_tokens %&gt;% tokens_tolower() %&gt;% tokens_remove(stopwords(&quot;english&quot;)) y #&gt; [1] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [5] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [9] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [13] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [17] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [21] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [25] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [29] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [33] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [37] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [41] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [45] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [49] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [53] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [57] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [61] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [65] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [69] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [73] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [77] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [81] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [85] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [89] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [93] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [97] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [101] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [105] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [109] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [113] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [117] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [121] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [125] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [129] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [133] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [137] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [141] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [145] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [149] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [153] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [157] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [161] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [165] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [169] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [173] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [177] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [181] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [185] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [189] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [193] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [197] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [201] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [205] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [209] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [213] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [217] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [221] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [225] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [229] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [233] Boris Johnson Boris Johnson Boris Johnson Boris Johnson #&gt; [237] Boris Johnson Boris Johnson Boris Johnson Macron #&gt; [241] Macron Macron Macron Macron #&gt; [245] Macron Macron Macron Macron #&gt; [249] Macron Macron Macron Macron #&gt; [253] Macron Macron Macron Macron #&gt; [257] Macron Macron Macron Macron #&gt; [261] Macron Macron Macron Macron #&gt; [265] Macron Macron Macron Macron #&gt; [269] Macron Macron Macron Macron #&gt; [273] Macron Macron Macron Macron #&gt; [277] Macron Macron Macron Macron #&gt; [281] Macron Macron Macron Macron #&gt; [285] Macron Macron Macron Macron #&gt; [289] Macron Macron Macron Macron #&gt; [293] Macron Macron Macron Macron #&gt; [297] Macron Macron Macron Macron #&gt; [301] Macron Macron Macron Macron #&gt; [305] Macron Macron Macron Macron #&gt; [309] Macron Macron Macron Macron #&gt; [313] Macron Macron Macron Macron #&gt; [317] Macron Macron Macron Macron #&gt; [321] Macron Macron Macron Macron #&gt; [325] Macron Macron Macron Macron #&gt; [329] Macron Macron Macron Macron #&gt; [333] Macron Macron Macron Macron #&gt; [337] Macron Macron Macron Macron #&gt; [341] Macron Macron Macron Macron #&gt; [345] Macron Macron Macron Macron #&gt; [349] Macron Macron Macron Macron #&gt; [353] Macron Macron Macron Macron #&gt; [357] Macron Macron Macron Macron #&gt; [361] Macron Macron Macron Macron #&gt; [365] Macron Macron Macron Macron #&gt; [369] Macron Macron Macron Macron #&gt; [373] Macron Macron Macron Macron #&gt; [377] Macron Macron Macron Macron #&gt; [381] Macron Macron Macron Macron #&gt; [385] Macron Macron Macron Macron #&gt; [389] Macron Macron Macron Macron #&gt; [393] Macron Macron Macron Macron #&gt; [397] Macron Macron Macron Macron #&gt; [401] Macron Macron Macron Macron #&gt; [405] Macron Macron Macron Macron #&gt; [409] Macron Macron Macron Macron #&gt; [413] Macron Macron Macron Macron #&gt; [417] Macron Macron Macron Macron #&gt; [421] Macron Macron Macron Macron #&gt; [425] Macron Macron Macron Macron #&gt; [429] Macron Macron Macron Macron #&gt; [433] Macron Macron Macron Macron #&gt; [437] Macron Macron Macron Macron #&gt; [441] Macron Macron Macron Macron #&gt; [445] Macron Macron Macron Macron #&gt; [449] Macron Macron Macron Macron #&gt; [453] Macron Macron Macron Macron #&gt; [457] Macron Macron Macron Macron #&gt; [461] Macron Macron Macron Macron #&gt; [465] Macron Macron Macron Macron #&gt; [469] Macron Macron Macron Macron #&gt; [473] Macron Macron Macron Macron #&gt; [477] Macron Macron Macron Macron #&gt; [481] Macron Macron Macron Macron #&gt; [485] Macron Macron Macron Macron #&gt; [489] Macron Macron Macron Macron #&gt; [493] Macron Macron Macron Macron #&gt; [497] Macron Macron Macron Macron #&gt; [501] Macron Macron Macron Macron #&gt; [505] Macron Macron Macron Macron #&gt; [509] Macron Macron Macron Macron #&gt; [513] Macron Macron Macron Macron #&gt; [517] Macron Macron Macron Macron #&gt; [521] Macron Macron Macron Macron #&gt; [525] Macron Macron Macron Macron #&gt; [529] Macron Macron Macron Macron #&gt; [533] Macron Macron Macron Macron #&gt; [537] Macron Macron Macron Macron #&gt; [541] Macron Macron Macron Macron #&gt; [545] Macron Macron Macron Macron #&gt; [549] Macron Macron Macron Macron #&gt; [553] Macron Macron Macron Macron #&gt; [557] Macron Macron Macron Macron #&gt; [561] Macron Macron Macron Macron #&gt; [565] Macron Macron Macron Macron #&gt; [569] Macron Macron Macron Macron #&gt; [573] Macron Macron Macron Macron #&gt; [577] Macron Macron Macron Macron #&gt; [581] Macron Macron Macron Macron #&gt; [585] Macron Macron Macron Macron #&gt; [589] Macron Macron Macron Macron #&gt; [593] Macron Macron Macron Macron #&gt; [597] Macron Macron Macron Macron #&gt; [601] Macron Macron Macron Macron #&gt; [605] Macron Macron Macron Macron #&gt; [609] Macron Macron Macron Macron #&gt; [613] Macron Macron Macron Macron #&gt; [617] Macron Macron Macron Macron #&gt; [621] Macron Macron Macron Macron #&gt; [625] Macron Macron Macron Macron #&gt; [629] Macron Macron Macron Macron #&gt; [633] Macron Macron Macron Macron #&gt; [637] Macron Macron Macron Macron #&gt; [641] Macron Macron Macron Macron #&gt; [645] Macron Macron Macron Macron #&gt; [649] Macron Macron Macron Macron #&gt; [653] Macron Macron Macron Macron #&gt; [657] Macron Macron Macron Macron #&gt; [661] Macron Macron Macron Macron #&gt; [665] Macron Macron Macron Macron #&gt; [669] Macron Macron Macron Macron #&gt; [673] Macron Macron Macron Macron #&gt; [677] Macron Macron Macron Macron #&gt; [681] Macron Macron Macron Macron #&gt; [685] Macron Macron Macron Macron #&gt; [689] Macron Macron Macron Macron #&gt; [693] Macron Macron Macron Macron #&gt; [697] Macron Macron Macron Macron #&gt; [701] Macron Macron Macron Macron #&gt; [705] Macron Macron Macron Macron #&gt; [709] Macron Macron Macron Macron #&gt; [713] Macron Macron Macron Macron #&gt; [717] Macron Macron Macron Macron #&gt; [721] Macron Macron Macron Macron #&gt; [725] Macron Macron Macron Macron #&gt; [729] Macron Macron #&gt; Levels: Boris Johnson Macron y&lt;-factor(docvars(combine_tokens,&quot;author&quot;)) Then, we build the featues. To this aim, we first compute the DTM matrix. combine.dfm&lt;-dfm(combine_tokens) combine.dfm #&gt; Document-feature matrix of: 771 documents, 1,691 features (99.4% sparse) and 3 docvars. #&gt; features #&gt; docs morning meeting government&#39;s cobr emergency committee #&gt; text1 0 0 0 0 0 0 #&gt; text2 1 0 1 1 1 1 #&gt; text3 0 0 0 0 0 0 #&gt; text4 0 0 0 0 0 0 #&gt; text5 0 0 0 0 0 0 #&gt; text6 0 0 0 0 0 0 #&gt; features #&gt; docs coronavirus outbreak first scotland #&gt; text1 0 0 0 0 #&gt; text2 1 1 0 0 #&gt; text3 0 0 3 1 #&gt; text4 0 0 0 0 #&gt; text5 0 0 0 0 #&gt; text6 1 0 0 0 #&gt; [ reached max_ndoc ... 765 more documents, reached max_nfeat ... 1,681 more features ] 8.1 LSA Because of the huge number of tokens, the feature matrix hence obtained may be too big to train a model in a reasonable amount of time. We thus apply a reduction dimension technoque in order to obtain less feature but still kepp the relevant informations. LSA is perfect to this. As a first trialm we target 30 dimensions ( 30 subjects) combine_corpus.dfm &lt;- dfm(combine_corpus) cmod&lt;-textmodel_lsa(combine_corpus.dfm,nd=30) 8.2 Random forest First we need to shape the data in a dataframe. We separate the text into sentence in order to have a more consistent and robust dataframe. Then we call for the training. In this simple context, in order to illustrate the concepts without too long computation times, we will limit ourselves to just one training set and one test set in a 08/02 pattern. df&lt;-data.frame(Class=y, x=cmod$docs) index.tr&lt;-sample(size = round(0.8*length(y)),x=c(1:length(y)),replace = FALSE) df.tr&lt;-df[index.tr,] df.te&lt;-df[-index.tr,] combine.fit&lt;-ranger(Class~., data = df.tr) pred.te&lt;-predict(combine.fit,df.te) In order to see the prediction quality of the model, we call the confusionMatrix function in the caret package confusionMatrix(data=pred.te$predictions,reference = df.te$Class) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Boris Johnson Macron #&gt; Boris Johnson 29 7 #&gt; Macron 30 88 #&gt; #&gt; Accuracy : 0.76 #&gt; 95% CI : (0.684, 0.825) #&gt; No Information Rate : 0.617 #&gt; P-Value [Acc &gt; NIR] : 0.000121 #&gt; #&gt; Kappa : 0.451 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.000298 #&gt; #&gt; Sensitivity : 0.492 #&gt; Specificity : 0.926 #&gt; Pos Pred Value : 0.806 #&gt; Neg Pred Value : 0.746 #&gt; Prevalence : 0.383 #&gt; Detection Rate : 0.188 #&gt; Detection Prevalence : 0.234 #&gt; Balanced Accuracy : 0.709 #&gt; #&gt; &#39;Positive&#39; Class : Boris Johnson #&gt; We see an accuracy of 69.9%. Also a sensitivity of 40.4% and a specificity of 86.2%. The prediction quality is not well balanced between the 2 class. The model struggle to predict the negative class, which is Macron. It is probably because we there is less data of Macron. 8.3 Improving the features nd.vec&lt;-c(2,5,25,50,100,500,1000) acc.vec&lt;-numeric(length(nd.vec)) for (j in 1:length(nd.vec)) { cmod&lt;-textmodel_lsa(combine_corpus.dfm,nd=nd.vec[j]) df&lt;-data.frame(class=y,x=cmod$docs) df.tr&lt;-df[index.tr,] df.te&lt;-df[-index.tr,] combine.fit&lt;-ranger(class~., data = df.tr) pred.te&lt;-predict(combine.fit,df.te) acc.vec[j]&lt;-confusionMatrix(data=pred.te$predictions,reference = df.te$class)$overall[1] } acc.vec #&gt; [1] 0.740 0.721 0.766 0.747 0.766 0.740 0.747 plot(acc.vec~nd.vec,type=&quot;b&quot;) We can see that 100 is the best choice among the ones we tried. combine_corpus.dfm &lt;- dfm(combine_corpus) cmod&lt;-textmodel_lsa(combine_corpus.dfm,nd=100) df&lt;-data.frame(class=y, x=cmod$docs) index.tr&lt;-sample(size = round(0.8*length(y)),x=c(1:length(y)),replace = FALSE) df.tr&lt;-df[index.tr,] df.te&lt;-df[-index.tr,] combine.fit&lt;-ranger(class~., data = df.tr) pred.te&lt;-predict(combine.fit,df.te) confusionMatrix(data=pred.te$predictions,reference = df.te$class) #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Boris Johnson Macron #&gt; Boris Johnson 37 4 #&gt; Macron 25 88 #&gt; #&gt; Accuracy : 0.812 #&gt; 95% CI : (0.741, 0.87) #&gt; No Information Rate : 0.597 #&gt; P-Value [Acc &gt; NIR] : 1.05e-08 #&gt; #&gt; Kappa : 0.586 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.000204 #&gt; #&gt; Sensitivity : 0.597 #&gt; Specificity : 0.957 #&gt; Pos Pred Value : 0.902 #&gt; Neg Pred Value : 0.779 #&gt; Prevalence : 0.403 #&gt; Detection Rate : 0.240 #&gt; Detection Prevalence : 0.266 #&gt; Balanced Accuracy : 0.777 #&gt; #&gt; &#39;Positive&#39; Class : Boris Johnson #&gt; We clearly improve the accruacy in adding more dimensions "],["conclusion.html", "Chapter 9 Conclusion", " Chapter 9 Conclusion "],["references.html", "References", " References "]]
