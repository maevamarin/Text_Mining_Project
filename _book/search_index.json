[
["index.html", "Dicours Analysis Chapter 1 Basic introduction", " Dicours Analysis Eugénie Mathieu, Maeva Marin, Hadrien Renger, Wajma Nazim 09 décembre, 2020 Chapter 1 Basic introduction "],
["introduction.html", "Chapter 2 Introduction 2.1 Overview and Motivation 2.2 Data loadind", " Chapter 2 Introduction 2.1 Overview and Motivation 2.2 Data loadind # Boris Johnson&#39;s speech of March 16th boris16mars &lt;- read_html(&quot;https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-16-march-2020&quot;)%&gt;% html_nodes(xpath=&quot;//*[@id=&#39;content&#39;]/div[3]/div[1]/div[1]/div[2]/div&quot;) %&gt;% html_text() macron &lt;- read_html(&quot;https://franceintheus.org/spip.php?article9654&quot;) %&gt;% html_nodes(&quot;div.texte&quot;) %&gt;% html_text() "],
["eda.html", "Chapter 3 EDA 3.1 Data Acquisition 3.2 Tokenisation, Lemmatization &amp; Cleaning 3.3 Document-Term Matrix DTM 3.4 Lexical Divesity Token Type Ratio TTR 3.5 yule’s index 3.6 MATTR", " Chapter 3 EDA 3.1 Data Acquisition To download the differents speaches we scrap the speaches on 2 differents websites. The speaches from Macron come from the website of the Embassy of France in Washington DC (USA) and regarding Boris Johnson speaches, it comes from the official website of the governement of the United-Kingdom. 3.1.1 Emmanuel Macron We choose the 3 first speaches from Macron about the corona virus dating from the: - 12 march (text1) - 16 march (text2) - 13 april (text3) summary(macron) Text Types Tokens Sentences text1 1007 3772 162 text2 917 2947 107 text3 1103 3730 107 The first speaches from Macron was quite long , 162 sentences and the two next were shorter: 107 sentences. Each speech consists approximatly of an average of 3200 words. 3.1.2 Boris Johnson We choose the 7 first speaches from president Johnson about the corona virus dating from the: - 09 march (text1) - 12 march (text2) - 16 march (text3) - 18 march (text5) - 19 march (text6) - 20 march (text7) - 22 march (text8) summary(boris) ###number of tokens and token types Text Types Tokens Sentences text1 266 577 21 text2 409 1170 49 text3 405 1165 42 text4 406 1175 51 text5 321 904 1 text6 357 946 42 text7 300 729 33 Johnson made more speeches but shorter. His first speech was 577 words, then the following ones ranged from 729 to 1175 words. 3.2 Tokenisation, Lemmatization &amp; Cleaning 3.2.1 Emmanuel Macron ## Tokenization corpus_macron &lt;- corpus(macron) corpus_macron &lt;- tokens(corpus_macron, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE) ## Lemmatization corpus_macron &lt;- tokens_replace(corpus_macron, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) ## Cleaning corpus_macron=corpus_macron %&gt;% tokens_tolower() %&gt;% tokens_remove(stopwords(&quot;english&quot;)) corpus_macron #&gt; Tokens consisting of 3 documents. #&gt; text1 : #&gt; [1] &quot;man&quot; &quot;woman&quot; &quot;france&quot; &quot;dear&quot; &quot;compatriot&quot; #&gt; [6] &quot;past&quot; &quot;week&quot; &quot;country&quot; &quot;confront&quot; &quot;spread&quot; #&gt; [11] &quot;virus&quot; &quot;covid-19&quot; #&gt; [ ... and 1,718 more ] #&gt; #&gt; text2 : #&gt; [1] &quot;woman&quot; &quot;man&quot; &quot;france&quot; &quot;thursday&quot; &quot;night&quot; #&gt; [6] &quot;speak&quot; &quot;health&quot; &quot;crisis&quot; &quot;country&quot; &quot;confront&quot; #&gt; [11] &quot;point&quot; &quot;epidemic&quot; #&gt; [ ... and 1,314 more ] #&gt; #&gt; text3 : #&gt; [1] &quot;frenchwoman&quot; &quot;frenchman&quot; &quot;dear&quot; &quot;compatriot&quot; #&gt; [5] &quot;live&quot; &quot;difficult&quot; &quot;times.we&quot; &quot;feel&quot; #&gt; [9] &quot;fear&quot; &quot;distress&quot; &quot;right&quot; &quot;now&quot; #&gt; [ ... and 1,690 more ] 3.2.2 Boris Johnson ## Tokenization corpus_boris &lt;- corpus(boris) corpus_boris &lt;- tokens(corpus_boris, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE) ## Lemmatization corpus_boris &lt;- tokens_replace(corpus_boris, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) ## Cleaning corpus_boris = corpus_boris %&gt;% tokens_tolower() %&gt;% tokens_remove(stopwords(&quot;english&quot;)) corpus_boris #&gt; Tokens consisting of 7 documents. #&gt; text1 : #&gt; [1] &quot;morning&quot; &quot;chair&quot; &quot;meet&quot; &quot;government&#39;s&quot; #&gt; [5] &quot;cobr&quot; &quot;emergency&quot; &quot;committee&quot; &quot;coronavirus&quot; #&gt; [9] &quot;outbreak&quot; &quot;first&quot; &quot;minister&quot; &quot;scotland&quot; #&gt; [ ... and 246 more ] #&gt; #&gt; text2 : #&gt; [1] &quot;good&quot; &quot;afternoon&quot; &quot;everybody&quot; &quot;thank&quot; #&gt; [5] &quot;much&quot; &quot;come&quot; &quot;just&quot; &quot;chair&quot; #&gt; [9] &quot;meet&quot; &quot;government&#39;s&quot; &quot;emergency&quot; &quot;committee&quot; #&gt; [ ... and 501 more ] #&gt; #&gt; text3 : #&gt; [1] &quot;good&quot; &quot;afternoon&quot; &quot;everybody&quot; &quot;thank&quot; &quot;much&quot; #&gt; [6] &quot;come&quot; &quot;want&quot; &quot;bring&quot; &quot;everyone&quot; &quot;date&quot; #&gt; [11] &quot;national&quot; &quot;fight&quot; #&gt; [ ... and 477 more ] #&gt; #&gt; text4 : #&gt; [1] &quot;good&quot; &quot;afternoon&quot; &quot;thank&quot; &quot;come&quot; &quot;indeed&quot; #&gt; [6] &quot;tune&quot; &quot;daily&quot; &quot;update&quot; &quot;want&quot; &quot;introduce&quot; #&gt; [11] &quot;sure&quot; &quot;know&quot; #&gt; [ ... and 532 more ] #&gt; #&gt; text5 : #&gt; [1] &quot;want&quot; &quot;begin&quot; &quot;thank&quot; &quot;everyone&quot; &quot;thank&quot; #&gt; [6] &quot;medium&quot; &quot;also&quot; &quot;thank&quot; &quot;everyone&quot; &quot;huge&quot; #&gt; [11] &quot;effort&quot; &quot;country&quot; #&gt; [ ... and 356 more ] #&gt; #&gt; text6 : #&gt; [1] &quot;good&quot; &quot;afternoon&quot; &quot;thank&quot; &quot;come&quot; &quot;today&quot; #&gt; [6] &quot;join&quot; &quot;chancellor&quot; &quot;exchequer&quot; &quot;rishi&quot; &quot;sunak&quot; #&gt; [11] &quot;jennie&quot; &quot;harry&quot; #&gt; [ ... and 392 more ] #&gt; #&gt; [ reached max_ndoc ... 1 more document ] 3.3 Document-Term Matrix DTM Now let’s compute the word frequencies (TF) by documents. First, the tokens are grouped by the indicator Document, which allows to count the words by documents. The the object is ungrouped. These frqeuencies are represented with barplots. We only keep the 15 most frequent words for redability purpose and create barplots using ggplot and geom_col. The facetwrap function split the barplots per Document. We see that the list of 15 most frequent term are due to doc 2 ,4 and 3. Now we want to know which are the most frequent term for each document 3.3.1 Emmanuel Macron ## Document-Term Matrix DTM corpus_macron.dfm &lt;- dfm(corpus_macron) macron_dtm &lt;- VectorSource(corpus_macron) %&gt;% VCorpus() %&gt;% DocumentTermMatrix(control=list(removePunctuation=TRUE, removeNumbers=TRUE, stopwords=TRUE)) macron_tidy &lt;- tidy(macron_dtm) datatable(macron_tidy, class = &quot;cell-border stripe&quot;) #top 15 macron_tidy %&gt;% group_by(term) %&gt;% summarise(&quot;count&quot;=sum(`count`)) %&gt;% top_n(15) %&gt;% ggplot(aes(x=count, y=term)) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;15 most common words in Macron&#39;s speeches&quot;) + xlab(&quot;Frequency&quot;) + ylab(&quot;Word&quot;) + geom_bar(stat = &quot;identity&quot;) + scale_y_reordered() #top 15 par texte macron_tidy %&gt;% group_by(document) %&gt;% top_n(15) %&gt;% ungroup() %&gt;% mutate(document = factor(as.numeric(document), levels = 1:17)) %&gt;% ggplot(aes(reorder_within(term, count, document), count, fill =term)) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;15 most common words in Macron&#39;s speech&quot;) + xlab(&quot;Word&quot;) + ylab(&quot;Frequency&quot;) + geom_bar(stat = &quot;identity&quot;) + scale_x_reordered() + coord_flip() + facet_wrap(~ document, scales = &quot;free&quot;) #top 16 mots plus utilisés par texte macron_count = macron_tidy %&gt;% group_by(term) %&gt;% summarise(&quot;count&quot;=sum(`count`)) macron_index = top_n(macron_count, 15) macron_tidy %&gt;% filter(term %in% macron_index$term) %&gt;% ggplot(aes(x=term, y = count)) + geom_col()+ coord_flip()+ facet_wrap(~document, ncol=2) 3.3.2 Boris Johnson ## Document-Term Matrix DTM corpus_boris.dfm &lt;- dfm(corpus_boris) boris_dtm &lt;- VectorSource(corpus_boris) %&gt;% VCorpus() %&gt;% DocumentTermMatrix(control=list(removePunctuation=TRUE, removeNumbers=TRUE, stopwords=TRUE)) boris_tidy &lt;- tidy(boris_dtm) datatable(boris_tidy, class = &quot;cell-border stripe&quot;) #top 15 boris_tidy %&gt;% group_by(term) %&gt;% summarise(&quot;count&quot;=sum(`count`)) %&gt;% top_n(15) %&gt;% ggplot(aes(x=count, y=term)) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;15 most common words in Boris Johnson&#39;s speeches&quot;) + xlab(&quot;Frequency&quot;) + ylab(&quot;Word&quot;) + geom_bar(stat = &quot;identity&quot;) + scale_y_reordered() #top 15 par texte boris_tidy %&gt;% group_by(document) %&gt;% top_n(15) %&gt;% ungroup() %&gt;% mutate(document = factor(as.numeric(document), levels = 1:17)) %&gt;% ggplot(aes(reorder_within(term, count, document), count, fill =term)) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;15 most common words in Boris Johnson&#39;s speeches&quot;) + xlab(&quot;Word&quot;) + ylab(&quot;Frequency&quot;) + geom_bar(stat = &quot;identity&quot;) + scale_x_reordered() + coord_flip() + facet_wrap(~ document, scales = &quot;free&quot;) #top 16 mots plus utilisés par texte boris_count = boris_tidy %&gt;% group_by(term) %&gt;% summarise(&quot;count&quot;=sum(`count`)) boris_index = top_n(boris_count, 15) boris_tidy %&gt;% filter(term %in% boris_index$term) %&gt;% ggplot(aes(x=term, y = count)) + geom_col()+ coord_flip()+ facet_wrap(~document, ncol=2) ## TF-IDF Now we repeat the same analysis using the TF-IDF. 3.3.3 Emmanuel Macron ## TFIDF no point when just on document, maybe add when combining texts corpus_macron.tfidf &lt;- dfm_tfidf(corpus_macron.dfm) #tfidf macron_index_tfidf = tidy(corpus_macron.tfidf) %&gt;% group_by(document) %&gt;% top_n(1) tidy(corpus_macron.tfidf) %&gt;% filter(term %in% macron_index_tfidf$term) %&gt;% ggplot( aes(term, count)) + geom_col()+ coord_flip()+ facet_wrap(~document, ncol=2) ###Boris Johnson ## TFIDF no point when just on document, maybe add when combining texts corpus_boris.tfidf &lt;- dfm_tfidf(corpus_boris.dfm) #tfidf boris_index_tfidf = tidy(corpus_boris.tfidf) %&gt;% group_by(document) %&gt;% top_n(1) tidy(corpus_boris.tfidf) %&gt;% filter(term %in% boris_index_tfidf$term) %&gt;% ggplot( aes(term, count)) + geom_col()+ coord_flip()+ facet_wrap(~document, ncol=2) ## Cloud of Words 3.3.4 Emmanuel Macron ## Cloud of Words textplot_wordcloud(corpus_macron.dfm, color=brewer.pal(8, &quot;Dark2&quot;)) textplot_wordcloud(corpus_macron.tfidf, color=brewer.pal(8, &quot;Dark2&quot;)) ### Boris Johnson ## Cloud of Words textplot_wordcloud(corpus_boris.dfm, color=brewer.pal(8, &quot;Dark2&quot;)) textplot_wordcloud(corpus_boris.tfidf, color=brewer.pal(8, &quot;Dark2&quot;)) 3.4 Lexical Divesity Token Type Ratio TTR 3.4.1 Emmanuel Macron ## Lexical Divesity Token Type Ratio TTR N.macron &lt;- ntoken(corpus_macron) V.macron &lt;- ntype(corpus_macron) TTR.macron &lt;- V.macron/N.macron TTR.macron ###the text is quite poor, as TTR is of 0.4 #&gt; text1 text2 text3 #&gt; 0.413 0.493 0.489 3.4.2 Boris Johnson ## Lexical Divesity Token Type Ratio TTR N.boris &lt;- ntoken(corpus_boris) V.boris &lt;- ntype(corpus_boris) TTR.boris &lt;- V.boris/N.boris TTR.boris ###the text is quite rich, as TTR is of 0.6 #&gt; text1 text2 text3 text4 text5 text6 text7 #&gt; 0.674 0.548 0.558 0.509 0.576 0.611 0.611 ##Zipf’s Law Now, we illustrate the Zipf’s law on the discous of Boris Jonhson. The terms are ranked by their frequency (rank=1 for the most frequent), then plotted versus its rank. This is easily obtained using quanteda. Now on the log scale this gives a linear relation 3.4.3 Emmanuel Macron corpus_macron_freq &lt;- textstat_frequency(corpus_macron.dfm) plot(log(frequency)~log(rank), data=corpus_macron_freq, pch=20) ggplot(corpus_macron_freq,aes(x = rank, y = frequency, label=feature)) + geom_point(size=2, alpha =1) + theme_bw() + geom_text(aes(label=feature),hjust=0, vjust=0) + xlim(0,20) ### Boris Johnson corpus_boris_freq &lt;- textstat_frequency(corpus_boris.dfm) plot(log(frequency)~log(rank), data=corpus_boris_freq, pch=20) ggplot(corpus_boris_freq,aes(x = rank, y = frequency, label=feature)) + geom_point(size=2, alpha =1) + theme_bw() + geom_text(aes(label=feature),hjust=0, vjust=0) + xlim(0,20) 3.5 yule’s index 3.5.1 Emmanuel Macron textstat_lexdiv(corpus_macron.dfm, measure = &quot;I&quot;) %&gt;% ggplot(aes(x=reorder(document,I), y=I))+ geom_point()+ coord_flip()+ xlab(&quot;Text&quot;)+ ylab(&quot;Yule&#39;s index&quot;) ### Boris Johnson textstat_lexdiv(corpus_boris.dfm, measure = &quot;I&quot;) %&gt;% ggplot(aes(x=reorder(document,I), y=I))+ geom_point()+ coord_flip()+ xlab(&quot;Text&quot;)+ ylab(&quot;Yule&#39;s index&quot;) 3.6 MATTR 3.6.1 Emmanuel Macron textstat_lexdiv(corpus_macron, measure = &quot;MATTR&quot;, MATTR_window = 10) %&gt;% ggplot(aes(x=reorder(document,MATTR), y=MATTR))+ geom_point()+ coord_flip()+ xlab(&quot;Text&quot;)+ ylab(&quot;MATTR&quot;) ### Boris Johnson textstat_lexdiv(corpus_boris, measure = &quot;MATTR&quot;, MATTR_window = 10) %&gt;% ggplot(aes(x=reorder(document,MATTR), y=MATTR))+ geom_point()+ coord_flip()+ xlab(&quot;Text&quot;)+ ylab(&quot;MATTR&quot;) "],
["similarities.html", "Chapter 4 Similarities 4.1 Boris 4.2 Macron 4.3 Comparison", " Chapter 4 Similarities library(readr) library(quanteda) library(knitr) library(kableExtra) library(reshape2) library(ggplot2) 4.1 Boris ## Jaccard Similarity boris.jac &lt;- textstat_simil(corpus_boris.tfidf, method = &quot;jaccard&quot;, margin = &quot;documents&quot;) ## Cosine Similarity boris.cos &lt;- textstat_simil(corpus_boris.tfidf, method = &quot;cosine&quot;, margin = &quot;documents&quot;) ## Euclidean Distance boris.euc &lt;- textstat_dist(corpus_boris.tfidf, method = &quot;euclidean&quot;, margin = &quot;documents&quot;) ## Jaccard Matrix boris.jac.mat &lt;- melt(as.matrix(boris.jac)) ggplot(data=boris.jac.mat, aes(x=Var1, y=Var2, fill=value)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint =0.5, limit=c(0,1), name=&quot;Jaccard&quot;) + geom_tile() ## Cosine Matrix boris.cos.mat &lt;- melt(as.matrix(boris.cos)) ggplot(data=boris.cos.mat, aes(x=Var1, y=Var2, fill=value)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint=0.5, limit=c(0,1), name=&quot;Cosine&quot;) + geom_tile() ## Euclidean Matrix boris.euc.mat &lt;- melt(as.matrix(boris.euc)) M &lt;- max(boris.euc.mat$value) boris.euc.mat$value.std &lt;- (M-boris.euc.mat$value)/M ggplot(data=boris.euc.mat, aes(x=Var1, y=Var2, fill=boris.euc.mat$value.std)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint=0.5, limit=c(0,1),name =&quot;Euclidean&quot;) + geom_tile() ## Clustering ## Jaccard Method boris.hc &lt;- hclust(dist(boris.euc)) boris.hc &lt;- hclust(dist(1 - boris.jac)) plot(boris.hc) ## Cosine Method boris.hc &lt;- hclust(dist(boris.euc)) boris.hc &lt;- hclust(dist(1 - boris.cos)) plot(boris.hc) ## Dendrogram = Hierarchical Clustering boris.clust &lt;- cutree(boris.hc, k=3) boris.clust #&gt; text1 text2 text3 text4 text5 text6 text7 #&gt; 1 2 2 2 3 3 3 ## K-means Method = Partitionning boris.km &lt;- kmeans(corpus_boris.tfidf, centers=3) boris.km$cluster #&gt; text1 text2 text3 text4 text5 text6 text7 #&gt; 2 2 1 3 2 2 2 ### Extracting the 10 most used words - Dendrogram data.frame( clust1 = names(sort(apply(corpus_boris.tfidf[boris.clust==1,],2,sum), decreasing = TRUE)[1:10]), clust2 = names(sort(apply(corpus_boris.tfidf[boris.clust==2,],2,sum), decreasing = TRUE)[1:10]), clust3 = names(sort(apply(corpus_boris.tfidf[boris.clust==3,],2,sum), decreasing = TRUE)[1:10]) ) clust1 clust2 clust3 outbreak school already chris child see four mass progress manage parent robert tackle period jenrick minister public behind phase ensure thousand thing important bite patrick dangerous virus delay london huge ### Extracting the 10 most used words - K-Means data.frame( clust1 = names(sort(apply(corpus_boris.tfidf[boris.km$cluster==1,],2,sum), decreasing = TRUE)[1:10]), clust2 = names(sort(apply(corpus_boris.tfidf[boris.km$cluster==2,],2,sum), decreasing = TRUE)[1:10]), clust3 = names(sort(apply(corpus_boris.tfidf[boris.km$cluster==3,],2,sum), decreasing = TRUE)[1:10]) ) clust1 clust2 clust3 mass outbreak school london already child contact see parent ensure chris pupil gathering dangerous teacher fight progress already without robert fightback stop jenrick update non-essential virus judgment rather public downward 4.2 Macron ## Jaccard Similarity macron.jac &lt;- textstat_simil(corpus_macron.tfidf, method = &quot;jaccard&quot;, margin = &quot;documents&quot;) ## Cosine Similarity macron.cos &lt;- textstat_simil(corpus_macron.tfidf, method = &quot;cosine&quot;, margin = &quot;documents&quot;) ## Euclidean Distance macron.euc &lt;- textstat_dist(corpus_macron.tfidf, method = &quot;euclidean&quot;, margin = &quot;documents&quot;) ## Jaccard Matrix macron.jac.mat &lt;- melt(as.matrix(macron.jac)) ggplot(data=macron.jac.mat, aes(x=Var1, y=Var2, fill=value)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint =0.5, limit=c(0,1), name=&quot;Jaccard&quot;) + geom_tile() ## Cosine Matrix macron.cos.mat &lt;- melt(as.matrix(macron.cos)) ggplot(data=macron.cos.mat, aes(x=Var1, y=Var2, fill=value)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint=0.5, limit=c(0,1), name=&quot;Cosine&quot;) + geom_tile() ## Euclidean Matrix macron.euc.mat &lt;- melt(as.matrix(macron.euc)) M &lt;- max(macron.euc.mat$value) macron.euc.mat$value.std &lt;- (M-macron.euc.mat$value)/M ggplot(data=macron.euc.mat, aes(x=Var1, y=Var2, fill=macron.euc.mat$value.std)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint=0.5, limit=c(0,1),name =&quot;Euclidean&quot;) + geom_tile() 4.3 Comparison "],
["topic-modelling.html", "Chapter 5 Topic Modelling 5.1 Boris Johnson 5.2 Macron 5.3 Comparison", " Chapter 5 Topic Modelling In this chapter, we analzye the topics of the speechs of Boris Jonhson and Macron using LSA and LDA. 5.1 Boris Johnson 5.1.1 LSA First, we make the DTM matrix. We are goin to use 3 dimensions. bmod&lt;-textmodel_lsa(corpus_boris.dfm,nd=3) To inspect the results, we can extract the matrices involved in the LSA decomposition head(bmod$docs) text1 -0.175 0.053 -0.148 text2 -0.519 0.052 -0.813 text3 -0.358 -0.737 0.101 text4 -0.525 0.626 0.342 text5 -0.299 -0.210 0.314 text6 -0.356 -0.121 0.226 head(bmod$features) morning -0.002 0.002 -0.005 government’s -0.011 0.005 -0.039 cobr -0.007 -0.023 -0.002 emergency -0.030 -0.050 -0.011 committee -0.009 0.004 -0.034 coronavirus -0.047 0.015 -0.026 Often the first dimension in LSA is associated with the document lenght. To see if it is true, we build a scatter-plot between the document lengt and Dimension 1. ns&lt;-apply(corpus_boris.dfm,1,sum) plot(ns~bmod$docs [,1]) We clearly observe that the dimension 1 is negatively correlated with the document lenght. Now in order to make the link between the topics and the documents and the topics with term, we use biplot biplot(y=bmod$docs[,2:3],x=bmod$features[,2:3], col=c(&quot;grey&quot;,&quot;red&quot;), xlab = &quot;Dimension 2&quot;, ylab=&quot;Dimension 3&quot;) We repeat the same analysis with TF-IDF bmod_2&lt;- textmodel_lsa(corpus_boris.tfidf, nd=3) head(bmod_2$docs) text1 -0.147 -0.063 -0.318 text2 -0.380 -0.187 -0.752 text3 -0.444 -0.750 0.463 text4 -0.735 0.615 0.256 text5 -0.180 -0.132 -0.173 text6 -0.200 -0.040 -0.153 head(bmod_2$features) morning -0.008 -0.004 -0.025 government’s -0.024 -0.014 -0.070 cobr -0.021 -0.037 0.007 emergency -0.042 -0.038 0.013 committee -0.019 -0.011 -0.054 coronavirus -0.016 -0.001 -0.011 5.1.2 LDA We now turn to the LDA. For illustration, we will make K=5 topis. K&lt;-5 corpus_boris.dtm&lt;- convert(corpus_boris.dfm, to=&quot;topicmodels&quot;) lda_boris&lt;- LDA(corpus_boris.dtm ,k=K) Top terms per topic and top topic per document can be easily obtained. Belo, the six top terms and the top topic are extracted. terms(lda_boris,6) Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 will will will now will go much school people want can can need go thank people disease make say much now now keep need measure know spread want can everyone topics(lda_boris,1) ## To see the topics related to each document #&gt; text1 text2 text3 text4 text5 text6 text7 #&gt; 2 2 4 3 1 1 5 We now build the bar plot to inspect the per-topic-per-word probabilities (beta’s). We take the 10 top terms and rearrange the beta per topic according to this order. beta.td.boris&lt;-tidy(lda_boris,matrix=&quot;beta&quot;) beta.top.term.boris&lt;-beta.td.boris %&gt;% group_by(topic) %&gt;% top_n(10, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) beta.top.term.boris topic term beta 1 will 0.028 1 go 0.019 1 can 0.017 1 people 0.014 1 now 0.013 1 know 0.013 1 want 0.012 1 get 0.012 1 day 0.012 1 disease 0.010 1 take 0.010 1 say 0.010 1 already 0.010 2 will 0.039 2 much 0.022 2 can 0.017 2 disease 0.014 2 now 0.013 2 spread 0.013 2 people 0.012 2 week 0.012 2 thing 0.010 2 time 0.010 2 good 0.010 3 will 0.039 3 school 0.022 3 need 0.017 3 make 0.017 3 keep 0.017 3 want 0.015 3 go 0.015 3 child 0.015 3 also 0.011 3 spread 0.011 3 far 0.011 3 already 0.011 4 now 0.027 4 people 0.014 4 go 0.014 4 say 0.014 4 need 0.014 4 can 0.012 4 ask 0.012 4 much 0.012 4 contact 0.012 4 disease 0.010 4 week 0.010 4 social 0.010 4 day 0.010 4 gathering 0.010 5 will 0.031 5 want 0.022 5 thank 0.022 5 much 0.022 5 measure 0.022 5 everyone 0.019 5 business 0.016 5 follow 0.016 5 slow 0.013 5 spread 0.013 5 advice 0.013 5 today 0.013 5 take 0.013 5 set 0.013 5 people 0.013 5 say 0.013 5 life 0.013 beta.top.term.boris %&gt;% mutate(term=reorder_within(term, beta, topic)) %&gt;% ggplot(aes(term,beta,fill=factor(topic))) + facet_wrap(~topic, scales = &quot;free&quot;) + coord_flip()+ scale_x_reordered() gamma.td.boris&lt;- tidy(lda_boris,matrix=&quot;gamma&quot;) gamma.td.boris document topic gamma text1 1 0 text2 1 0 text3 1 0 text4 1 0 text5 1 1 text6 1 1 text7 1 0 text1 2 1 text2 2 1 text3 2 0 text4 2 0 text5 2 0 text6 2 0 text7 2 0 text1 3 0 text2 3 0 text3 3 0 text4 3 1 text5 3 0 text6 3 0 text7 3 0 text1 4 0 text2 4 0 text3 4 1 text4 4 0 text5 4 0 text6 4 0 text7 4 0 text1 5 0 text2 5 0 text3 5 0 text4 5 0 text5 5 0 text6 5 0 text7 5 1 gamma.td.boris %&gt;% ggplot(aes(document,gamma,fill=factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~topic,scales = &quot;free&quot;)+ coord_flip()+ scale_x_reordered() 5.2 Macron 5.2.1 LSA mmod&lt;-textmodel_lsa(corpus_macron.dfm,nd=3) To inspect the results, we can extract the matrices involved in the LSA decomposition head(mmod$docs) text1 -0.666 0.502 0.552 text2 -0.426 0.351 -0.834 text3 -0.612 -0.791 -0.020 head(mmod$features) france -0.080 0.032 0.009 dear -0.054 0.016 -0.022 past -0.023 -0.017 0.037 country -0.075 -0.107 -0.071 spread -0.049 0.077 0.024 virus -0.101 -0.015 0.085 Often the first dimension in LSA is associated with the document lenght. To see if it is true, we build a scatter-plot between the document lengt and Dimension 1. ns_macron&lt;-apply(corpus_macron.dfm,1,sum) plot(ns_macron~mmod$docs [,1]) We clearly observe that the dimension 1 is negatively correlated with the document lenght. Now in order to make the link between the topics and the documents and the topics with term, we use biplot biplot(y=mmod$docs[,2:3],x=mmod$features[,2:3], col=c(&quot;grey&quot;,&quot;red&quot;), xlab = &quot;Dimension 2&quot;, ylab=&quot;Dimension 3&quot;) We repeat the same analysis with TF-IDF mmod_2&lt;- textmodel_lsa(corpus_macron.tfidf, nd=3) head(mmod_2$docs) text1 -0.244 0.921 0.304 text2 -0.107 0.286 -0.952 text3 -0.964 -0.265 0.028 head(mmod_2$features) france 0.000 0.000 0.000 dear 0.000 0.000 0.000 past -0.042 0.028 0.016 country 0.000 0.000 0.000 spread 0.000 0.000 0.000 virus 0.000 0.000 0.000 5.2.2 LDA We now turn to the LDA. For illustration, we will make K=5 topis. K&lt;-5 corpus_macron.dtm&lt;- convert(corpus_macron.dfm, to=&quot;topicmodels&quot;) lda_macron&lt;- LDA(corpus_macron.dtm ,k=K) lda_macron #&gt; A LDA_VEM topic model with 5 topics. Top terms per topic and top topic per document can be easily obtained. Belo, the six top terms and the top topic are extracted. terms(lda_macron,6) Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 will will will will will much know much also must continue even take must virus government much also much may week now even day come able work health us can topics(lda_macron,1) ## To see the topics related to each document #&gt; text1 text2 text3 #&gt; 3 4 1 We now build the bar plot to inspect the per-topic-per-word probabilities (beta’s). We take the 10 top terms and rearrange the beta per topic according to this order. beta.td.macron&lt;-tidy(lda_macron,matrix=&quot;beta&quot;) beta.top.term.macron&lt;-beta.td.macron %&gt;% group_by(topic) %&gt;% top_n(10, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) beta.top.term.macron topic term beta 1 will 0.021 1 much 0.018 1 continue 0.011 1 government 0.010 1 week 0.010 1 able 0.009 1 us 0.009 1 country 0.009 1 can 0.008 1 work 0.007 2 will 0.038 2 know 0.018 2 even 0.016 2 much 0.014 2 now 0.013 2 work 0.013 2 vulnerable 0.012 2 care 0.012 2 action 0.012 2 also 0.012 3 will 0.040 3 much 0.015 3 take 0.013 3 also 0.012 3 even 0.009 3 health 0.009 3 protect 0.009 3 virus 0.009 3 time 0.008 3 make 0.008 4 will 0.029 4 also 0.016 4 must 0.014 4 much 0.011 4 day 0.009 4 us 0.008 4 protect 0.008 4 one 0.008 4 even 0.008 4 go 0.007 5 will 0.079 5 must 0.016 5 virus 0.016 5 may 0.015 5 come 0.013 5 can 0.012 5 rule 0.012 5 day 0.012 5 many 0.012 5 help 0.011 beta.top.term.macron %&gt;% mutate(term=reorder_within(term, beta, topic)) %&gt;% ggplot(aes(term,beta,fill=factor(topic))) + facet_wrap(~topic, scales = &quot;free&quot;) + coord_flip()+ scale_x_reordered() gamma.td.macron&lt;- tidy(lda_macron,matrix=&quot;gamma&quot;) gamma.td.macron document topic gamma text1 1 0.000 text2 1 0.000 text3 1 0.638 text1 2 0.185 text2 2 0.000 text3 2 0.000 text1 3 0.815 text2 3 0.000 text3 3 0.000 text1 4 0.000 text2 4 1.000 text3 4 0.000 text1 5 0.000 text2 5 0.000 text3 5 0.362 gamma.td.macron %&gt;% ggplot(aes(document,gamma,fill=factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~topic,scales = &quot;free&quot;)+ coord_flip()+ scale_x_reordered() 5.3 Comparison "],
["word-embedding.html", "Chapter 6 Word Embedding 6.1 Boris Johnson 6.2 Macron 6.3 Comparison", " Chapter 6 Word Embedding 6.1 Boris Johnson Here, we compute the co-occurence matrix. We use the fcm function from quanteda. We use a window lenght 5. speech.coo.boris&lt;-fcm(corpus_boris,context=&quot;window&quot;,window=5, tri=FALSE) p&lt;-2 #word embedding dimension speech.glove.boris&lt;-GlobalVectors$new(rank = p,x_max = 10) #xmas is a neede technical option speech.weC.boris&lt;-speech.glove.boris$fit_transform(speech.coo.boris) #&gt; INFO [17:51:46.617] epoch 1, loss 0.0339 #&gt; INFO [17:51:46.699] epoch 2, loss 0.0241 #&gt; INFO [17:51:46.717] epoch 3, loss 0.0223 #&gt; INFO [17:51:46.720] epoch 4, loss 0.0211 #&gt; INFO [17:51:46.723] epoch 5, loss 0.0202 #&gt; INFO [17:51:46.726] epoch 6, loss 0.0194 #&gt; INFO [17:51:46.729] epoch 7, loss 0.0185 #&gt; INFO [17:51:46.733] epoch 8, loss 0.0178 #&gt; INFO [17:51:46.736] epoch 9, loss 0.0171 #&gt; INFO [17:51:46.739] epoch 10, loss 0.0166 For illustration purpose, we now plot the 50 most used terms n.w.boris&lt;-apply(corpus_boris.dfm,2,sum) #compute the number of times each term is used index&lt;-order(n.w.boris,decreasing = TRUE)[1:50] plot(speech.weC.boris[index,],type = &quot;n&quot;,xlab = &quot;Dimension 1&quot;, ylab = &quot;Dimendion 2&quot;) text(x=speech.weC.boris[index,],labels = rownames(speech.weC.boris[index,])) speech.dtm &lt;- corpus_boris.dfm speech.rwmd.model.boris&lt;-RelaxedWordMoversDistance$new(corpus_boris.dfm,speech.weC.boris) speech.rwms.boris&lt;-speech.rwmd.model.boris$sim2(corpus_boris.dfm) speech.rwmd.boris&lt;-speech.rwmd.model.boris$dist2(corpus_boris.dfm) speech.hc.boris&lt;-hclust(as.dist(speech.rwmd.boris)) plot(speech.hc.boris,cex=0.8) We can observe that there is some coherence within the groups in terms the date of the speech. speech.cl.boris&lt;- cutree(speech.hc.boris,k=4) corpus_boris.dfm[speech.cl.boris==1,] #&gt; Document-feature matrix of: 2 documents, 797 features (71.5% sparse). #&gt; features #&gt; docs morning government&#39;s cobr emergency committee coronavirus #&gt; text1 1 2 1 1 1 3 #&gt; text2 0 1 0 1 1 2 #&gt; features #&gt; docs outbreak first scotland minister #&gt; text1 5 4 1 3 #&gt; text2 0 0 1 1 #&gt; [ reached max_nfeat ... 787 more features ] 6.2 Macron speech.coo.macron&lt;-fcm(corpus_macron,context=&quot;window&quot;,window=5, tri=FALSE) p&lt;-2 #word embedding dimension speech.glove.macron&lt;-GlobalVectors$new(rank = p,x_max = 10) #xmas is a neede technical option speech.weC.macron&lt;-speech.glove.macron$fit_transform(speech.coo.macron) #&gt; INFO [17:51:47.869] epoch 1, loss 0.0246 #&gt; INFO [17:51:47.879] epoch 2, loss 0.0176 #&gt; INFO [17:51:47.887] epoch 3, loss 0.0157 #&gt; INFO [17:51:47.921] epoch 4, loss 0.0145 #&gt; INFO [17:51:47.931] epoch 5, loss 0.0136 #&gt; INFO [17:51:47.948] epoch 6, loss 0.0127 #&gt; INFO [17:51:47.958] epoch 7, loss 0.0121 #&gt; INFO [17:51:47.967] epoch 8, loss 0.0115 #&gt; INFO [17:51:47.977] epoch 9, loss 0.0110 #&gt; INFO [17:51:47.991] epoch 10, loss 0.0107 For illustration purpose, we now plot the 50 most used terms n.w.macron&lt;-apply(corpus_macron.dfm,2,sum) #compute the number of times each term is used index&lt;-order(n.w.macron,decreasing = TRUE)[1:50] plot(speech.weC.macron[index,],type = &quot;n&quot;,xlab = &quot;Dimension 1&quot;, ylab = &quot;Dimendion 2&quot;) text(x=speech.weC.macron[index,],labels = rownames(speech.weC.macron[index,])) speech.dtm.macron &lt;- corpus_macron.dfm speech.rwmd.model.macron&lt;-RelaxedWordMoversDistance$new(corpus_macron.dfm,speech.weC.macron) speech.rwms.macron&lt;-speech.rwmd.model.macron$sim2(corpus_macron.dfm) speech.rwmd.macron&lt;-speech.rwmd.model.macron$dist2(corpus_macron.dfm) speech.hc.macron&lt;-hclust(as.dist(speech.rwmd.macron)) plot(speech.hc.macron,cex=0.8) We can observe that there is some coherence within the groups in terms the date of the speech. speech.cl.macron&lt;- cutree(speech.hc.macron,k=2) corpus_macron.dfm[speech.cl.macron==1,] #&gt; Document-feature matrix of: 2 documents, 1,469 features (47.4% sparse). #&gt; features #&gt; docs france dear past country spread virus covid-19 several #&gt; text1 10 6 3 4 8 13 4 5 #&gt; text3 7 5 3 12 2 11 1 5 #&gt; features #&gt; docs thousand fellow #&gt; text1 2 4 #&gt; text3 1 1 #&gt; [ reached max_nfeat ... 1,459 more features ] 6.3 Comparison "],
["conclusion.html", "Chapter 7 Conclusion", " Chapter 7 Conclusion "],
["references.html", "References", " References "]
]
