# EDA

## Boris Johnson

## Macron

## Comparison
# Exploratory Data Analysis John Boris

## Data manipulation
```{r include=FALSE}
# Data Acquisition
boris16mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-16-march-2020") %>%
                html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>% 
                html_text()

boris12mars<- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-12-march-2020") %>%   
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()

boris18mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-18-march-2020") %>%   
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()


boris9mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-9-march-2020") %>% 
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()


boris19mars<- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-19-march-2020") %>% 
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()

boris20mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-20-march-2020") %>% 
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()


boris22mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-22-march-2020") %>% 
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()


BORIS<-corpus(c(boris9mars,boris12mars,boris16mars,boris18mars,boris19mars,boris20mars,boris22mars))
boris


```

```{r  include=TRUE, warning=FALSE}
boris16mars_dtm <- VectorSource(boris16mars) %>% VCorpus() %>%  DocumentTermMatrix(control=list(removePunctuation=TRUE, removeNumbers=TRUE, stopwords=TRUE))
boris16mars_tidy <- tidy(boris16mars_dtm)
boris16mars_tidy %>%
  group_by(document) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(document = factor(as.numeric(document), levels = 1:17)) %>%
  ggplot(aes(drlib::reorder_within(term, count, document), count, fill =term)) +
  theme(legend.position = "none") +
  ggtitle("15 most common words in Boris Johnson's speech on March 16th") +
  xlab("Word") + ylab("Frequency") +
  geom_bar(stat = "identity") +
  drlib::scale_x_reordered() +
  coord_flip() +
  facet_wrap(~ document, scales = "free")
```


```{r Boris tokenization and frequency}
## tokenization of the speech of Boris Johnson
boris <- tibble(txt = boris16mars)
boris16mars.fr<- boris %>% unnest_tokens(word, txt) %>%
  anti_join(stop_words, by = "word") %>%group_by(word) %>% summarize(Count =n()) %>% 
  arrange(desc(Count)) 
boris %>%
  unnest_tokens(word, txt) %>%
  anti_join(stop_words, by = "word") %>%group_by(word) %>% summarize(Count =n()) %>% 
  arrange(desc(Count)) %>%
  kable() %>% kable_styling()
```


```{r Full spring speech Boris Cleaning and DTM}
BORIS.cp <- VCorpus(VectorSource(boris))
BORIS.cp <- tm_map(BORIS.cp, removePunctuation) 
BORIS.cp <- tm_map(BORIS.cp, removeNumbers)
BORIS.cp <- tm_map(BORIS.cp, removeWords, stopwords("english"))
BORIS.cp <- tm_map(BORIS.cp, content_transformer(tolower))
BORIS.cp <- tm_map(BORIS.cp, stripWhitespace)
BORIS.dtm <- DocumentTermMatrix(BORIS.cp) %>% tidy()
BORIS.tfidf <- DocumentTermMatrix(BORIS.cp, control = list(weighting = weightTfIdf))
inspect(BORIS.tfidf)
```



```{r}
#create a tibble
Document <- c("doc1","doc2","doc3","doc4","doc5","doc6","doc7")
Text<-c(boris9mars,boris12mars,boris16mars,boris18mars,boris19mars,boris20mars,boris22mars)
boris<-data.frame(Document,Text)


# Now we remove the stop words using the anti_join function and the built-in stop_word object.That will remove unimportant word like the "a","the" etc...
boris.tok<-anti_join(boris.tok,stop_words,by="word")

#Number of token in each document
table(boris.tok$Document)

```

Now let's compute the word frequencies (TF) by documents. First, the tokens are grouped by the indicator Document, which allows to count the words by documents. The the object is ungrouped.

```{r}
boris.fr<- boris.tok %>% group_by(Document) %>% count(word,sort = TRUE) %>% ungroup()

```

These frqeuencies are represented with barplots. We only keep the 15 most frequent words for redability purpose and create barplots using ggplot and geom_col. The facetwrap function split the barplots per Document.

```{r}
index<-top_n(boris.fr,15)
boris.fr%>%
filter(word %in% index$word) %>%
ggplot( 
  aes(x=word,y=n))+
  geom_col() +
  coord_flip() +
  facet_wrap(~Document,ncol=2)

boris%>% group_by(Document) %>% count(Text,sort=TRUE) %>%ungroup()

```

We see that the list of 15 most frequent term are due to doc 2 ,4 and 3. 

Now we want to know which are  the most frequent term for each document

```{r}
index<-boris.fr %>% group_by(Document) %>% top_n(1)
boris.fr%>%
filter(word %in% index$word) %>%
ggplot( 
  aes(x=word,y=n))+
  geom_col() +
  coord_flip() +
  facet_wrap(~Document,ncol=2)
```


Now we repeat the same analysis using the TF-IDF.

```{r}
boris.tfidf<- bind_tf_idf(tbl=boris.fr,term=word,document=Document,n=n)
index<-boris.tfidf %>% group_by(Document) %>% top_n(1)
boris.tfidf%>%
filter(word %in% index$word) %>%
ggplot( 
  aes(x=word,y=n))+
  geom_col() +
  coord_flip() +
  facet_wrap(~Document,ncol=2)

```
We can see that the term "measures" or "disease" appaear several times. It is normal since the could of words was built from the frequencies within each document. We need first to aggregate the frequencies per termms(i.e sum iver the documents). Then er produce the could

```{r,warning=FALSE}
wordcloud(words=boris.fr$word,freq=boris.fr$n)

```

```{r,warning=FALSE}
boris.fr2<-aggregate(n~word,FUN=sum,data = boris.fr)
wordcloud(words=boris.fr2$word,freq=boris.fr2$n)



```

## Zipf's law

Now, we illustrate the Zipf's law on the discous of Boris Jonhson. The terms are ranked by their frequency (rank=1 for the most frequent), then plotted versus its rank. This is easily obtained using quanteda.

```{r,warning=FALSE}
boris.dfm<- dfm(boris.tok$word)
boris.freq <- textstat_frequency(boris.dfm)
plot(frequency~rank,data=boris.freq,pch=20)
text(frequency~rank, data=boris.freq[1:5],label=feature,pos=4)
```

Now on the log scale this gives a linear relation

```{r,warning=FALSE}
plot(log(frequency)~log(rank),data=boris.freq,pch=20)
text(log(frequency)~log(rank), data=boris.freq[1:5],label=feature,pos=4)
```



