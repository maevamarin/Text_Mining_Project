# EDA

## Data Acquisition

To download the differents speaches we scrap the speaches on 2 differents websites.
The speaches from Macron come from the website of the Embassy of France in Washington DC (USA) and regarding Boris Johnson speaches, it comes from the official website of the governement of the United-Kingdom.

### Emmanuel Macron

We choose the 3 first speaches from Macron about the corona virus dating from the:
* 12 march (text1)
* 16 march (text2)
* 13 april (text3)

```{r include=FALSE}
# Data Acquisition Macron 

macron12march <- read_html("https://franceintheus.org/spip.php?article9654") %>%
  html_nodes("div.texte") %>%
  html_text()
macron12march <- str_replace_all(macron12march,"[\r\n\t]", "")
macron12march <- substr(macron12march, 178, 20197)

macron16march <- read_html("https://franceintheus.org/spip.php?article9659#1") %>%
  html_nodes("div.texte") %>%
  html_text()
macron16march <- macron16march <- str_replace_all(macron16march,"[\r\n\t]", "")
macron16march <- substr(macron16march, 131, 15719)

macron13april <- read_html("https://franceintheus.org/spip.php?article9710") %>%
  html_nodes("div.texte") %>%
  html_text() 
macron13april <- macron13april <- str_replace_all(macron13april,"[\r\n\t]", "")
macron13april <- substr(macron13april, 117, 20000)


macron <- corpus(c(macron12march,macron16march,macron13april))
```

```{r, echo=FALSE}
knitr::kable(summary(macron), caption = "Macron's speeches" ,align = "lccrr",digits = 4)
```

The first speaches from Macron was quite long , 162 sentences and the two next were shorter: 107 sentences. Each speech consists approximatly of an average of 3200 words.

### Boris Johnson

We choose the 7 first speaches from president Johnson about the corona virus dating from the:
  * 09 march (text1)
  * 12 march (text2)
  * 16 march (text3)
  * 18 march (text5)
  * 19 march (text6)
  * 20 march (text7)
  * 22 march (text8)

```{r include=FALSE}

# Data Acquisition Boris

boris16mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-16-march-2020") %>%
                html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>% 
                html_text()

boris12mars<- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-12-march-2020") %>%   
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()

boris18mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-18-march-2020") %>%   
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()

boris9mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-9-march-2020") %>% 
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()

boris19mars<- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-19-march-2020") %>% 
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()

boris20mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-20-march-2020") %>% 
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()


boris22mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-22-march-2020") %>% 
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()


boris<-corpus(c(boris9mars,boris12mars,boris16mars,boris18mars,boris19mars,boris20mars,boris22mars))
```

```{r, echo=FALSE}
knitr::kable(summary(boris), caption = "Johnson's speeches" ,align = "lccrr",digits = 4)
```

Johnson made more speeches but shorter. His first speech was 577 words, then the following ones ranged from 729 to 1175 words.

## Tokenisation, Lemmatization & Cleaning

The numbers, the punctuation,the symbols and the separators are remove.
As well as unimportant words. Moreover we cast all letters to their lower option.

### Emmanuel Macron

```{r, echo=FALSE}

## Tokenization
corpus_macron <- corpus(macron)
corpus_macron <- tokens(corpus_macron, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE)

## Lemmatization
corpus_macron <- tokens_replace(corpus_macron, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma)

## Cleaning
corpus_macron=corpus_macron %>% 
  tokens_tolower() %>% 
  tokens_remove(stopwords("english")) 

corpus_macron

```

### Boris Johnson

```{r, echo=FALSE}

## Tokenization
corpus_boris <- corpus(boris)
corpus_boris <- tokens(corpus_boris, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE)

## Lemmatization
corpus_boris <- tokens_replace(corpus_boris, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma)

## Cleaning
corpus_boris = corpus_boris %>% 
  tokens_tolower() %>% 
  tokens_remove(stopwords("english")) 

corpus_boris

```

## Document-Term Matrix DTM

Now let's compute the word frequencies (TF) by documents. First, the tokens are grouped by the indicator Document, which allows to count the words by documents. 

### Table

*Emmanuel Macron*

```{r, echo=FALSE, warning=FALSE}
## Document-Term Matrix DTM
corpus_macron.dfm <- dfm(corpus_macron)

macron_dtm <- VectorSource(corpus_macron) %>% VCorpus() %>%  DocumentTermMatrix(control=list(removePunctuation=TRUE, removeNumbers=TRUE, stopwords=TRUE))
macron_tidy <- tidy(macron_dtm)

datatable(macron_tidy, class = "cell-border stripe")

```

*Boris Johnson*

```{r, echo=FALSE, warning=FALSE}

## Document-Term Matrix DTM
corpus_boris.dfm <- dfm(corpus_boris)

boris_dtm <- VectorSource(corpus_boris) %>% VCorpus() %>%  DocumentTermMatrix(control=list(removePunctuation=TRUE, removeNumbers=TRUE, stopwords=TRUE))
boris_tidy <- tidy(boris_dtm)

datatable(boris_tidy, class = "cell-border stripe")
```

### Most frequent words

We only keep the 15 most frequent words for redability purpose and create barplots using ggplot and geom_col. The facetwrap function split the barplots per Document.

#### All text confused

*Emmanuel Macron*

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#top 15 
macron_tidy %>%
  group_by(term) %>%
  summarise("count"=sum(`count`)) %>%
  top_n(15)  %>%
  ggplot(aes(x=count, y=term, fill = term)) +
  theme(legend.position = "none") +
  ggtitle("15 most common words in Macron's speeches") +
  xlab("Frequency") + ylab("Words") +
  geom_bar(stat = "identity") +
  scale_y_reordered() 

#top 16 mots plus utilisés par texte
macron_count  = macron_tidy %>%
  group_by(term) %>%
  summarise("count"=sum(`count`))

macron_index = top_n(macron_count, 15)

macron_tidy %>% filter(term %in% macron_index$term) %>%
  ggplot(aes(x=term, y = count, fill =term)) +
  geom_col()+
  ggtitle("Details of the more frequent words") +
  xlab("Words") + ylab("Frequency") +
  coord_flip()+
  facet_wrap(~document, ncol=2) + 
   guides(fill=FALSE, color=FALSE) 
```

*Boris Johnson*

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#top 15
boris_tidy %>%
  group_by(term) %>%
  summarise("count"=sum(`count`)) %>%
  top_n(15)  %>%
  ggplot(aes(x=count, y=term, fill =term)) +
  theme(legend.position = "none") +
  ggtitle("15 most common words in Johnson's speeches") +
  xlab("Frequency") + ylab("Words") +
  geom_bar(stat = "identity") +
  scale_y_reordered() 

#top 16 mots plus utilisés par texte
boris_count  = boris_tidy %>%
  group_by(term) %>%
  summarise("count"=sum(`count`))

boris_index = top_n(boris_count, 15)

boris_tidy %>% filter(term %in% boris_index$term) %>%
   ggplot(aes(x=term, y = count, fill=term)) +
  geom_col()+
  ggtitle("Details of the more frequent words") +
  xlab("Words") + ylab("Frequency") +
  coord_flip()+
  facet_wrap(~document, ncol=2)+ 
   guides(fill=FALSE, color=FALSE)
```

We see that the list of 15 most frequent term are due to doc 2 ,4 and 3. 

#### Per text 

Now we want to know which are the most frequent term for each speach.

*Emmanuel Macron*

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#top 15 par texte 
macron_tidy %>%
  group_by(document) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(document = factor(as.numeric(document), levels = 1:17)) %>%
  ggplot(aes(reorder_within(term, count, document), count, fill =term)) +
  theme(legend.position = "none") +
  ggtitle("15 most common words in each speach of Macron") +
  xlab("Words") + ylab("Frequency") +
  geom_bar(stat = "identity") +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~ document, scales = "free")
```

*Boris Johnson*

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#top 15 par texte
boris_tidy %>%
  group_by(document) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(document = factor(as.numeric(document), levels = 1:17)) %>%
  ggplot(aes(reorder_within(term, count, document), count, fill =term)) +
  theme(legend.position = "none") +
  ggtitle("15 most common words in Boris Johnson's speeches") +
  xlab("Words") + ylab("Frequency") +
  geom_bar(stat = "identity") +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~ document, scales = "free")
```


## TF-IDF

Now we repeat the same analysis using the TF-IDF.

### Emmanuel Macron

```{r, echo=FALSE, warning=FALSE, message=FALSE}
## TFIDF no point when just on document, maybe add when combining texts
corpus_macron.tfidf <- dfm_tfidf(corpus_macron.dfm)

#tfidf
macron_index_tfidf = tidy(corpus_macron.tfidf) %>% group_by(document) %>% top_n(1)

tidy(corpus_macron.tfidf) %>% filter(term %in% macron_index_tfidf$term) %>%
  ggplot( aes(term, count, fill=term)) +
  xlab("Words") + ylab("Frequency") +
  ggtitle("Most specific word for each Emmanuel Macron's speeches") +
  geom_col()+
  coord_flip()+
  facet_wrap(~document, ncol=2)+ 
   guides(fill=FALSE, color=FALSE)

```

### Boris Johnson

```{r, echo=FALSE, warning=FALSE, message=FALSE}
## TFIDF no point when just on document, maybe add when combining texts
corpus_boris.tfidf <- dfm_tfidf(corpus_boris.dfm)

#tfidf
boris_index_tfidf = tidy(corpus_boris.tfidf) %>% group_by(document) %>% top_n(1)

tidy(corpus_boris.tfidf) %>% filter(term %in% boris_index_tfidf$term) %>%
  ggplot( aes(term, count,fill=term)) +
  xlab("Words") + ylab("Frequency") +
  ggtitle("Most specific word for each Boris Johnson's speeches") +
  geom_col()+
  coord_flip()+
  facet_wrap(~document, ncol=2)+ 
   guides(fill=FALSE, color=FALSE)

```

## Cloud of Words

### Usind DFM

*Emmanuel macron*

```{r, echo=FALSE, warning=FALSE, message=FALSE}
textplot_wordcloud(corpus_macron.dfm, color=brewer.pal(8, "Dark2"))
```

*Boris Johnson*

```{r, echo=FALSE, warning=FALSE, message=FALSE}
textplot_wordcloud(corpus_boris.dfm, color=brewer.pal(8, "Dark2"))
```

### Using  TF-IDF

*Emmanuel macron*

```{r, echo=FALSE, warning=FALSE, message=FALSE}
textplot_wordcloud(corpus_macron.tfidf, color=brewer.pal(8, "Dark2"))
```

*Boris Johnson*

```{r, echo=FALSE, warning=FALSE, message=FALSE}
textplot_wordcloud(corpus_boris.tfidf, color=brewer.pal(8, "Dark2"))
```

## Lexical Divesity Token Type Ratio TTR

A TTR is comprised beetween o and 1.When equal to 1 it correspond to a rich diversity, every token is from a different type. In opposit if equal to 0, it mean a poor diversity ( he use only one word).

### Emmanuel Macron 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
N.macron <- ntoken(corpus_macron)
V.macron <- ntype(corpus_macron)
TTR.macron <- V.macron/N.macron
knitr::kable(TTR.macron, caption = "Lexical diversity of Macron." ,align = "lccrr",digits = 4) ###the text is quite poor, as TTR is of 0.4

```

Macron have a mean TTR of 0,45 which is quite poor.

### Boris Johnson

```{r, echo=FALSE, warning=FALSE, message=FALSE}
## Lexical Divesity Token Type Ratio TTR
N.boris <- ntoken(corpus_boris)
V.boris <- ntype(corpus_boris)
TTR.boris <- V.boris/N.boris
knitr::kable(TTR.boris, caption = "Lexical diversity of Johnson." ,align = "lccrr",digits = 4) ###the text is quite rich, as TTR is of 0.6

```

Johnson have quite a rich vocabulary, a mean of 0,6.

## Zipf's Law

Now, we illustrate the Zipf's law on the discours. The terms are ranked by their frequency (rank=1 for the most frequent), then plotted versus its rank. This is easily obtained using quanteda.

Thanks on the log scale this gives a linear relation.

### Emmanuel Macron 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
corpus_macron_freq <- textstat_frequency(corpus_macron.dfm)


ggplot(corpus_macron_freq,aes(x = rank, y = frequency, label=feature)) + geom_point(size=2, alpha =1) + theme_bw() + geom_text(aes(label=feature),hjust=0, vjust=0) + xlim(0,20)

plot(log(frequency)~log(rank), data=corpus_macron_freq, pch=20)
```

### Boris Johnson

```{r, echo=FALSE, warning=FALSE, message=FALSE}
corpus_boris_freq <- textstat_frequency(corpus_boris.dfm)

ggplot(corpus_boris_freq,aes(x = rank, y = frequency, label=feature)) + geom_point(size=2, alpha =1) + theme_bw() + geom_text(aes(label=feature),hjust=0, vjust=0) + xlim(0,20)

plot(log(frequency)~log(rank), data=corpus_boris_freq, pch=20)
```

## Yule's index

A larger index mean more diversity.

### Emmanuel Macron 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
textstat_lexdiv(corpus_macron.dfm, measure = "I") %>% 
  ggplot(aes(x=reorder(document,I), y=I))+
  geom_point()+
  coord_flip()+
  xlab("Text")+
  ylab("Yule's index")
```

### Boris Johnson

```{r, echo=FALSE, warning=FALSE, message=FALSE}
textstat_lexdiv(corpus_boris.dfm, measure = "I") %>% 
  ggplot(aes(x=reorder(document,I), y=I))+
  geom_point()+
  coord_flip()+
  xlab("Text")+
  ylab("Yule's index")
```


## MATTR

It is the Moving Average Type-Token Ratio 

### Emmanuel Macron 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
textstat_lexdiv(corpus_macron, measure = "MATTR", MATTR_window = 10)  %>%
  ggplot(aes(x=reorder(document,MATTR), y=MATTR))+
  geom_point()+
  coord_flip()+
  xlab("Text")+
  ylab("MATTR")
```

### Boris Johnson

```{r, echo=FALSE, warning=FALSE, message=FALSE}
textstat_lexdiv(corpus_boris, measure = "MATTR", MATTR_window = 10)  %>%
  ggplot(aes(x=reorder(document,MATTR), y=MATTR))+
  geom_point()+
  coord_flip()+
  xlab("Text")+
  ylab("MATTR")

```
